---
layout: post
bigtitle:  "A-NeRF:"
subtitle:   "Articulated Neural Radiance Fields for Learning Human Shape, Appearance, and Pose"
categories:
    - blog
    - papers
    - paper-etc
tags:
    - servey
comments: true
published: true
related_posts:
---
# A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape, Appearance, and Pose

2021 [paper](https://papers.nips.cc/paper/2021/file/65fc9fb4897a89789352e211ca2d398f-Paper.pdf)

Shih-Yang Su   
Frank Yu   
Michael Zollhöfer   
Helge Rhodin1

---

* toc
{:toc}


## Abstract

Deep learning은 classical motion capture pipeline을 feed-forward networks로 다시 만드는 동안, generative models는 iterative refinement를 통해 fine alignment를 recover하는 것을 요구받는다.
불행하게도, 기존 모델들은 보통 hand-crafted되거나 또는 통제된 조건에서 learned되고 오로지 제한된 domains에서 적용가능하다. 우리는 unlabelled monocular videos로부터 확장된 Neural Radiance Fields (NeRFs)를 통해 generative neural body model을 학습하는 method를 제안한다. 우리는 그것들에 시시각각 움직임(time-varying)과 관절운동(articulated motion)에 적용할 skeleton(골격)을 갖추었다.

Key insight는 implicit models가 explicit surface models에 시용되는 the inverse of the forward kinematics를 요구한다는것이다.
우리의 reparameterization은 body parts의 pose에 관련된 spatial latent variables를 정의하고 그것에 의해 overparameterization을 갖는 ill-posed inverse operations를 극복한다.
Jointly하게 articulated pose를 refining하는동안 이것은 밑바닥부터 volumetric body shape과 appearance를 학습하게 해준다; input videos에 대해 appearance, pose, 또는 3D shape에 대해 전부 ground truth labels가 없다. Novel-view-synthesis와 motion capture에 대해 사용할때, 우리의 neural model은 다양한 datasets에 대해 정확성을 향상시킨다.

## 1 Introduction

Generative models는 image를 재생성하는 Generative Adversarial Networks (GANs)에서 structured latent variables를 통해 downstream tasks에 대해 이해하는 control과 image를 제공하는 neural scene representations로 차츰 발전해오고 있다.
그러나, 대부분 3D models는 natural images에서 3D labels를 요청하고 dedicated depth sensors를 요청하다.  
따라서 2D observations로부터 3D representations를 학습하는것은 중요한 연구문제이다, 이것은 특히 사람의 다양한 body shapes 그리고 appearances 그리고 non-rigid motion에 대한 어려움이 있다.

Modern human motion capture techniques는 전형적으로 discriminative approach와 generative approach의 장점을 combine한다.
Feed-forward 3D human pose estimation approach는 human pose의 rough initial estimate를 제공한다. 이후, person의 high-quality 3D scan 또는 laser scans로부터 학습된 parametric human body model 중 하나를 기반으로 하는 generative approach는 image evidence 기반으로 iteratively하게 estimate를 refine한다. 비록 기존 모델들은 유례없는 정확도를 성취하였지만, 기존 모델들은 user의 low-dimensional, restrictive, shape body model 또는 personalized 3D scan 을 요구한다.

우리는 unlabelled videos에서 user-specific neural 3D body model을 learning하고 skeleton pose를 이해하는 Articulated Neural Radiance Fields (A-NeRF)를 소개한다. 이것을 motion capture에 적용할때, template models의 필요성을 줄이는 동시에 현재 generative approaches의 장점과 정확성을 유지한다.
A-NeRF는 Neural Radiance Fields (NeRF)를 single videos와 articulated motion 작업으로 확장한다.
A-NeRF는 scene을 implicitly(절대적으로) 다음과같이 parameterize한다.

$$F_{\phi}(\Gamma(\mathbf{q}),\Gamma(\mathbf{d})) \mapsto (\sigma,\mathbf{c}), \qquad \text{with} \; \sigma \in \mathbb{R}, \mathbf{c} \in \mathbb{R}^3, \mathbf{q} \in \mathbb{R}^3, \; \text{and} \; \mathbf{d} \in \mathbb{R}^3$$  


$$F_{\phi}$$와 Multi-layer Perceptron (MLP) with $$\Gamma$$, Positional Encoding (PE)를 chaining함으로써.
첫째, PE는 input scene point $$\mathbf{q}$$와 view direction $$\mathbf{d}$$를 higher dimensional space로 mapping한다. Higher dimensional space는 공간상의 모든 point에 radiance c와 opacity $$\sigma$$를 순차적으로 output하는 meaningful scene representation function $$F_{\phi}$$를 MLP가 학습할 수 있게 해준다.

둘째, (query locations에 대한 conditioning을 통해) implicitly described scene이 computer graphics로부터 classical ray-marching을 통해 rendered된다. MLP representation의 장점은 그것이 volumetric grids의 complexity를 피하고, screen-space convolution의 implicit bias에 의해 일어나는 artifacts를 회피하고 surface meshes와 달리, flexible topology를 갖을 수 있다. 그러나, original NeRF는 오직 multiple views에서 보이는 각 3D point와 같은 calibrated cameras의 dozens로부터 포착되는 static scenes에만 작동된다.

![Fig1](/assets/img/Blog/papers/A-NeRF/Fig1.PNG)
_Figure1 : 우리의 A-NeRF는 jointly하게 user의 neural body model을 learn하고 다양한 body poses에 작동한다(a). 또한 tedious camera calibration없이 single 또는 가능하면 multi-views에서 initial 3D articulated skeleton pose estimate를 refining한다(b). 기본에는 template free neural representation(c)과 volume volumetric rendering과 결합된 skeleton-based embedding이 있다._


우리의 개념적 contribution은 articulated skeleton과 관련된 neural latent representation을 학습하는것에 놓여있다.
인기있는 SMPL body model과 같은 explicit models는 forwards kinematics를 통해 surface를 변형시키는 반면, A-NeRF의 implicit form은 우리에서 어떻게 skeletons가 어떻게 integrated될 수 있는지를 다시 생각하게 해준다 – implicit networks는 3D world coordinates부터 완전히 탐구되지 않은 상당히 어려운 task인 reference skeleton까지 inverse transformation을 요구한다.
우리의 core technical novelty는 articulated skeleton에 관련된 local coordinates를 만들어내기 위해 수식 1에서 $$\Gamma(q)$$, $$\Gamma(d)$$의 different parameterizations을 찾아내는것이고 평가하는것이다. 3D world coordinates에 point는 body part에 유일하게 연결될수 없기 때문에 우리는 뼈당 하나로 embedding에 overparameterizing에 의한 mentioned ill-posed inverse problem을 해결한다.
이것은 인간 움직임 방법의 도메인 지식을 embed하고 neural network가 entire captured sequence에 걸쳐 body shape과 appearance constraints를 combine하는 common frame을 제공한다(그림2 참고).

우리는 우리의 모든 contributions가 이전에 parametric surface models 또는 multi-view approaches을 통해서만 얻었던 세부 수준에 도달하는 초기 rough한 3D pose estimates만 필요로하는 monocular video에서 neural body model을 학습할 수 있다는 것을 증명한다.


**Scope(범위/한계)**.
우리는 모델을 motion capture, character animation, 그리고 appearance 그리고 motion transfer에 적용하고 pose refinement가 existing monocular skeleton reconstruction에 제공됨을 시연한다.
우리는 transductive setting에서 training time은 알지만 ground-truth는 가지고 있지 않는, specific target video에 대해 learn한다. A-NeRF는 non-physical illumination인 그럴듯한 dynamic motions의 	novel view synthesis를 할수 있다. relighting applications을 할려면 추가적인 steps가 필요하다.

**General impact**
Personalized human body modeling의 self-supervised approach를 구축하면 supervised datasets에 잘 represented되지 않는 사람과 활동에 더 많이 포괄될 수 있다.
그러나, 동의 없이 사람의 3D models이 만들어질 수 있다는 위험은 감수하고 있다.
우리는 사용자가 오로지 motion capture algorithms를 개발하고 평가하기위해 수집된 datasets만 사용할 것을 권한다.

## 2 Related Work

우리의 approach는 human pose, shape estimation, human modeling, 그리고  neural scene representations에 대한 다음 연구를 기반으로 하며 이와 관련이 있다.

**Discriminative Human Pose Estimation.**  
3D joint positions 또는 skeleton의 joint angles와 bone length [56,77]의 feed-forward estimation은 상당히 정확한 반면, generalization gap때문에 input image에 overlayed될때 discriminative estimates는 misalignment하는 경향이 있다.  

skeleton pose는 2D pose estimates에 잘 match되게 refine 될 수 있지만, 이것은 보통 3D에 large errors를 야기한다 [39,40].  

우리는 skeleton pose를 초기화하는데 사용하고 그것을 neural body model과 combine한다.

**Surface-based Generative Body Models**  
Surface-based Generative Body Models는 deformation energies를 통해 template meshes에 제약을 가하는것(constraining) 또는 laser scans의 large collection으로부터 parametric human body models를 learning하는것 둘 중 하나로부터 얻어진다.  

그것들의 low-dimensional parameters는 그럴듯한 human shapes와 motions의 공간에 제약을 가한다(constrain). 이것은 single images, detailed texturing과 displacement mapping [4, 6], 그리고 alleviates manual rigging [3]로부터 real-time reconstructions를 가능하게해준다 [8,18].  
그것은 또한 differentiable form [30]로 neural training processes[5,23,26,45,48,68]에 통합될 때 학습된 prior [14,19,28]과 weak-supervision의 범위안에서 optimization이 가능하게 한다.

이 범주에서 우리의 approach에 가장 가까운 것은 untextured parametric quadruped model을 얼룩말 이미지로 textures하고 기하학적으로 refines하는 [79]와 optical flow을 사용하여 human pose를 refine하는 [72]의 model fitting methods이다.

비록 비슷한 setting이라도, 우리의 surface-free neural body model과 volumetric rendering은 forward kinematics가 갖춰져있고 a-priori가 얻어질 필요가 있는 textured triangle mesh와 근본적으로 다르다.

**Implicit Body Models.**  
A-NeRF는 differentiable ray-tracing을 통해 human pose, shape, 그리고 appearance를 refining 하는데 사용되는 level-sets[61] 와 density of a sum of Gaussians [21, 51, 52]의 측면에서 암묵적(implicitly)으로 정의된 body models와 유사성을 가지고 있다.

**Neural Scene Representations**

최근 neural scene representations는 mesh [31,66], point clouds [2,41,71], sphere sets [27], 그리고 dense volumetric grids [32,58]의 low-dimensional nonlinear representations을 학습한다.  
그것들의 respective geometric output representations은 classical rendering techniques를 사용하여 rendering이 가능하게 하지 expressiveness에 제약이 있다, 예를들어, surface mesh의 고정된 connectivity와 discretized(이산화된) volumes의 large memory footprint 때문이있다. 이 한계는 3D 공간에서 임의의 point를 특징짓기 위해서 positional encoding[63]과 쌍을 이루는 unconstrained MLPs [42]를 사용함으로써 극복되었다.

sphere tracing [59]으로 rendered되는 MLP의 level-sets를 통한 surface definitions와 ray-marching을 통해 rendered되는 density representations [42]이 일반적이다.   

rendering step은 maximum likelihood estimation를 위해, 3D model를 learning하는동안 관찰되는 variables -실제 이미지 -에 걸친 likelihood를 정의하기위해 필요하다.

rendering 역시 학습될 수 있다[43, 53, 54] 하지만 보통 불일치(inconsistencies)가 발생한다, 특히 training data가 희박할때.

몇몇 동시대의 연구들은 또한 camera motion [75], video reenactment [50], facial models [15, 16]을 refining 하기 위해 neural scene representations을 사용한다.

이런 연구들과 직교하게, 우리의 A-NeRF는 estimated poses와 uncalibrated cameras로부터 articulated body model을 학습한다.


우리의 것과 밀접한 관련이 있는 NASA surface body model [13]은 implicit function을 skeleton의 뼈에 견고(rigidly)하게 부착되는 individual implicit functions의 최소로 정의하며, 각각 모델 의존성에 대한 전체 human pose에 따라 조절되고 3D scans로부터 학습된다.

반대로, 우리는 surface model대신에 volumetric model를 학습하고 appearance와 rendering을 포함한다.

훨씬 더 비슷한것은 최근 NeuralBody[49] representation인데 이것은 NeRF를 surface body model과 기본 skeleton과 결합한다.

두 approaches와 대조적으로, 우리는 surface supervision이나 initialization, condition pose differently, 그리고 refine pose 이 필요하지 않고 우리가 unconstrained environments에서의 single videos로부터 학습할 수 있게 해준다.

## Formulation

![Fig2](/assets/img/Blog/papers/A-NeRF/Fig2.PNG)
_Figure2 : **Overview**. A-NeRF는 rendered될 수 있고 photometric loss $\mathcal{L}^{SV}$에 optimized 될 수 있는 generative model이다(white). 첫쨰, skeleton pose는 기존의 estimator에의해 initialized 된다(orange). 둘째, 이 pose는 skeleton-relative embedding를 통해 refine된다(blue). 그리고 그것이 NeRF에 넣어진다(green), 그리고 ray-marching에 의해 rendered되는 implicit body model을 이끌어낸다(red). skeleton-relative embedding의 key property는 single 3D query location이 overcomplete reparameterization에 mapping한다, 각 skeleton bone에 관련하여 represented된 같은 point와 함께(right)._


**Objective**

같은 사람에 대한 하나 또는 여려개이 비디오에서 나오는 images $$\mathbf{I}_k \in \mathbb{R}^{H \times W \times 3} \;$$  N개인 sequence $$[\mathbf{I}]^N_{k=1}$$를 고려하면, 우리의 goal은 시시각각 변하는 skeleton poses $$[\theta]^N_{k=1}$$을 동시에 estimate하고 상세한 body model을 learn한다.  
우리의 A-NeRF body model $$C_{\phi}$$는 volumetric shape과 color를 정의하는 neural network parameters $$\phi$$에 의해 parametrized된다.

그림2는 generative model의 overview를 보여준다.

이것은 보이지 않는 poses에 virtual body model의 rendering을 가능하게 하고 image reconstruction objective에 그것의 parameters $$\theta$$와 $$\phi$$가 optimize한다.

$$\mathcal{L}^{SV}(\theta,\phi) = \sum_k ||C_{\phi}(\theta_k) -\mathbf{I}_k||_1 + \lambda_{\theta} d(\theta_k - \hat{\theta_k}) + \lambda_t \left \Vert \frac{\partial^2 \theta_k}{\partial t^2} \right \Vert ^2_2 \tag{2}$$

data term, pose regularizer, smoothness prior

hyperparameters $$\lambda_t$$와 $$\lambda_\theta$$에 의해 균형잡힌 3개 terms 모두의 influence과 함께.

+ data term은 $$C_{\phi}$$에 의해 generated된 images들과 input image 사이의 distance를 L1 distance로 측정한다

+ pose regularizer는 solution이 기존 predictor [26]에 의해 얻은 initial pose estimate $\hat{\theta}$에 가깝게 해준, 허용되는 small shifts $\epsilon = 0.01$ with $d(x) = \text{min}(||x||^2_2 - \epsilon,0)$.

+ smoothness prior은 연속 frames의 poses간의 acceleration $\frac{\partial^2 \theta_k}{\partial t^2}$에 패널티를 준다.
수식2를 최소화 하는것은 quadratic energy terms가 Gaussian distributions의 log-likelihoods인 corresponding probabilistic model를 최대화 하는것으로 볼수 있다.

우리의 focus는 neural body model을 수식화하는것이다. 간단하게, 우리는 stochastic gradient descent하면서 inference하는동안 사용되는 objective functions에 관련된 방정식을 계속해서 쓴다.

### 3.1 NeRF and A-NeRF Image Formation Model

scene을 a collection of triangles이나 other primitives로 modeling 하는것 대신에, 우리는 neural network에 의해 human implicitly를 공간의 가능한 모든 3D points들과 view directions에 정의된 function(수식1)로 정의한다 [42].  

NeRF와 유사하게, 우리는 ray marching을 통해 human subject의 image를 render한다.

$$C_{\phi}(u,v;\theta_k) = \sum^Q_{i=1}T_i(1 - exp(-\sigma_i\delta_i))\mathbf{c}_i, \quad T_i=\text{exp} \left (-\sum^{i=1}_{j=1} \sigma_j \delta_j \right), \tag{3}$$

여기서 $$(u,v)$$는 image에서 2D pixel location이고 $\mathbf{d}$에 따라 샘플된 3D query positions $$\mathbf{q}_i$$까지 index i, 그리고 neighboring samples간의 거리인 $$\delta_i$$ - a constant if samples would be taken at regular intervals.  
$$T_i$$는 near plane부터 $$\mathbf{q}_i$$-fraction of light reaching the sensor from sample point $i$까지 ray traveling에 대한 accumulated transmittance(누적 투과율 )이다.  
$$c_i$$는 $$i$$에서 방출(emitted)되거나 반사(reflected)된 light color이다.  
최종 pixel color는 모든 Q samples들에 걸친, background의 special role을 취하는 마지막 sample로 모두 sum한것이다.
background color는 static camera setups에서 전체 비디오에 걸친 median pixel color를 통해 쉽게 추론된다.
그다음, 우리는 우리이 skeleton parametrization $$\theta_k$$와 어떻게 이것을 dynamic articulated human motion에 효과적으로 모델링하는데 사용할 수 있는지를 소개한다.

### 3.2 Articulated Skeleton Pose Model
