---
layout: post
bigtitle:  "Point-GNN"
subtitle:   ": Graph Neural Network for 3D Object Detection in a Point Cloud"
categories:
    - blog
    - papers
tags:
    - point-cloud
    - detection
comments: true
published: true
---



#Point-GNN : Graph Neural Network for 3D Object Detection in a Point Cloud

CVPR 2020 [paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Point-GNN_Graph_Neural_Network_for_3D_Object_Detection_in_a_CVPR_2020_paper.pdf)

39회 인용

* toc
{:toc}

## Abstract

In this paper, we propose a graph neural network to detect objects from a LiDAR point cloud.  
Towards this end, we encode the point cloud efficiently in a fixed radius near-neighbors graph.  
We design a graph neural network, named Point-GNN, to predict the category and shape of the object that each vertex in the graph belongs to.  

> 본 논문에서는 LiDAR point cloud에서 개체를 감지하기 위한 graph neural network을 제안한다.  
이를 위해, 우리는 고정된 radius near-neighbors graph에서 point cloud를 효율적으로 인코딩한다.  
우리는 그래프의 각 정점(vertex)이 속하는 객체의 범주와 모양을 예측하기 위해 Point-GNN이라는 graph neural network을 설계한다.

In Point-GNN, we propose an auto-registration mechanism to reduce translation variance, and also design a box merging and scoring operation to combine detections from multiple vertices accurately.  
Our experiments on the KITTI benchmark show the proposed approach achieves leading accuracy using the point cloud alone and can even surpass fusion-based algorithms.  
Our results demonstrate the potential of using the graph neural network as a new approach for 3D object detection.  

> Point-GNN에서는 translation variance을 줄이기 위한 auto-registration mechanism을 제안하고, 여러 정점(multiple vertices)에서 검출된 것을 정확하게 결합하기 위한 상자 병합(box merging) 및 채점 연산(scoring operation)을 설계한다.  
KITTI 벤치마크에 대한 우리의 실험은 제안된 접근 방식이 point cloud만을 사용하여 선도적인 정확도를 달성하고 융합 기반 알고리즘도 능가할 수 있음을 보여준다.   
우리의 결과는 graph neural network을 3D object detection를 위한 새로운 접근법으로 사용할 가능성을 보여준다.

The code is available at
https://github.com/WeijingShi/Point-GNN.

## 1. Introduction

Understanding the 3D environment is vital in robotic perception.  
A point cloud that composes a set of points in space is a widely-used format for 3D sensors such as LiDAR.  
Detecting objects accurately from a point cloud is crucial in applications such as autonomous driving.

> robotic perception에서는 3D environment을 이해하는 것이 매우 중요하다.  
공간의 point 집합을 구성하는 point cloud은 LiDAR과 같은 3D 센서에 널리 사용되는 형식이다.  
point cloud에서 물체를 정확하게 감지하는 것은 자율 주행과 같은 애플리케이션에서 중요하다.  

Convolutional neural networks that detect objects from images rely on the convolution operation.  
While the convolution operation is efficient, it requires a regular grid as input.  
Unlike an image, a point cloud is typically sparse and not spaced evenly on a regular grid.   
Placing a point cloud on a regular grid generates an uneven number of points in the grid cells.   
Applying the same convolution operation on such a grid leads to potential information loss in the crowded cells or wasted computation in the empty cells.  

> 이미지에서 물체를 감지하는 Convolutional neural networks은 convolution operation에 의존한다.   
convolution operation은 효율적이지만 입력으로 정규 그리드가 필요하다.  
이미지와 달리, point cloud은 일반적으로 sparse하고 일반 그리드에 고르게 띄어쓰지 않는다.  
point cloud을 일반 그리드에 배치하면 그리드 셀에서 불균일한 수의 point가 생성된다.  
그러한 그리드에 동일한 convolution 연산을 적용하면 crowded cells에서 잠재적 정보 손실(potential information loss) 또는 빈 셀에서 낭비되는 계산으로 이어진다.  


Recent breakthroughs in using neural networks [3] [22] allow an unordered set of points as input.  
Studies take advantage of this type of neural network to extract point cloud features without mapping the point cloud to a grid.  
However, they typically need to sample and group points iteratively to create a point set representation.  
The repeated grouping and sampling on a large point cloud can be computationally costly.  
Recent 3D detection approaches [10] [21] [16] often take a hybrid approach to use a grid and a set representation in different stages.  
Although they show some promising results, such hybrid strategies may suffer the shortcomings of both representations.

> 신경망을 사용하는 최근의 발전[3] [22]은 정렬되지 않은 points 집합을 입력으로 허용한다.   
연구는 이러한 유형의 neural network을 활용하여 point cloud를 그리드에 매핑하지 않고 point cloud features을 추출한다.  
그러나 일반적으로 point set representation을 만들려면 point을 반복적으로 sampling하고 grouping해야 한다.  
large point cloud에서 반복된 grouping 및 sampling은 계산 비용이 많이 들 수 있다.  
최근의 3D 감지 접근법[10] [21] [16]은 종종 다른 단계에서 grid 및 set representation을 사용하기 위해 hybrid 접근법을 채택한다.  
비록 몇 가지 유망한 결과를 보여주지만, 그러한 hybrid 전략은 두 가지 표현의 단점을 겪을 수 있다.  

In this work, we propose to use a graph as a compact representation of a point cloud and design a graph neural network called Point-GNN to detect objects.  
We encode the point cloud natively in a graph by using the points as the graph vertices.  
The edges of the graph connect neighborhood points that lie within a fixed radius, which allows feature information to flow between neighbors.  
Such a graph representation adapts to the structure of a point cloud directly without the need to make it regular.  
A graph neural network reuses the graph edges in every layer, and avoids grouping and sampling the points repeatedly.

> 본 연구에서는 graph를  point cloud의 compact representation으로 사용하고 객체를 감지하기 위해 Point-GNN이라는 graph neural network 설계를 제안한다.  
우리는 point을 graph 정점으로 사용하여 graph에서 point cloud을 기본적으로 인코딩한다.  
graph의 edges는 고정 반지름 내에 있는 neighborhood points을 연결하므로, 인접 영역 간에 feature information가 흐를 수 있다.  
이러한 graph representation은 정기적으로 설정할 필요 없이 포인트 클라우드의 구조에 직접 적응한다.  
graph neural network은 모든 layer에서 graph edges를 재사용하고 점의 grouping 및 sampling을 반복적으로 추출하지 않는다.  

Studies [15] [9] [2] [17] have looked into using graph neural network for the classification and the semantic segmentation of a point cloud.  
However, little research has looked into using a graph neural network for the 3D object detection in a point cloud.  
Our work demonstrates the feasibility of using a GNN for highly accurate object detection in a point cloud.

> 연구[15] [9] [2] [17]은 point cloud의 classification 및 semantic segmentation을 위해 graph neural network 사용을 조사했다.  
그러나 point cloud에서 3D object detection를 위한 graph neural network 사용에 대한 연구는 거의 없었다.  
우리의 연구는 point cloud에서 매우 정확한 객체 탐지를 위해 GNN을 사용할 가능성을 보여준다.  

Our proposed graph neural network Point-GNN takes the point graph as its input.  
It outputs the category and bounding boxes of the objects to which each vertex belongs.   
Point-GNN is a one-stage detection method that detects multiple objects in a single shot.    
To reduce the translation variance in a graph neural network, we introduce an auto-registration mechanism which allows points to align their coordinates based on their features.  
We further design a box merging and scoring operation to combine detection results from multiple vertices accurately.

> 제안된 graph neural network Point-GNN은 point graph를 입력으로 삼는다.   
각 정점이 속하는 객체의 category 및  bounding boxes를 출력한다.  
Point-GNN은 single shot으로 여러 개체를 감지하는 one-stage detection method이다.  
graph neural network의 변환 분산(translation variance)을 줄이기 위해, 우리는 점들이 특징에 따라 좌표를 align(정렬)할 수 있는 auto-registration mechanism을 도입한다.  
우리는 여러 정점에서 검출 결과를 정확하게 결합하기 위해 box merging 및 scoring operation을 추가로 설계한다.  

We evaluate the proposed method on the KITTI benchmark.  
On the KITTI benchmark, Point-GNN achieves the state-of-the-art accuracy using the point cloud alone and even surpasses sensor fusion approaches.  
Our Point-GNN shows the potential of a new type 3D object detection approach using graph neural network, and it can serve as a strong baseline for the future research.  
We conduct an extensive ablation study on the effectiveness of the components in Point-GNN.

> 제안된 방법을 KITTI 벤치마크에서 평가한다.  
KITTI 벤치마크에서 Point-GNN은 point cloud만을 사용하여 state-of-the-art accuracy를 달성하고 센서 융합 접근 방식까지 능가한다.  
우리의 Point-GNN은 graph neural network을 이용한 새로운 유형의 3D 객체 감지 접근법의 잠재력을 보여주며, 향후 연구를 위한 강력한 baseline으로 작용할 수 있다.  
우리는 Point-GNN에서 components의 효과에 대한 광범위한 절제 연구를 수행한다.  

In summery, the contributions of this paper are:  
- We propose a new object detection approach using graph neural network on the point cloud.  
- We design Point-GNN, a graph neural network with an auto-registration mechanism that detects multiple objects in a single shot.  
- We achieve state-of-the-art 3D object detection accuracy in the KITTI benchmark and analyze the effectiveness of each component in depth.

> - point cloud에서 graph neural network을 이용한 새로운 객체 감지 접근 방식을 제안한다.  
> - 우리는 single shot으로 여러 개체를 감지하는 auto-registration mechanism이 있는 graph neural network Point-GNN을 설계한다.  
> - KITTI 벤치마크에서 state-of-the-art 3D object detection accuracy를 달성하고 depth에서 각 component의 효과를 분석한다.

## 2. Related Work
Prior work in this context can be grouped into three categories, as shown in Figure 1.  

![Fig1](/assets/img/Blog/papers/PointGNN/Fig1.JPG)

**Point cloud in grids.**  
Many recent studies convert a point cloud to a regular grid to utilize convolutional neural networks.   
[20] projects a point cloud to a 2D Bird’s Eye View (BEV) image and uses a 2D CNN for object detection.  
[4] projects a point cloud to both a BEV image and a Front View (FV) image before applying a 2D CNN on both.  
Such projection induces a quantization error due to the limited image resolution.  
Some approaches keep a point cloud in 3D coordinates.  
[23] represents points in 3D voxels and applies 3D convolution for object detection.  
When the resolution of the voxels grows, the computation cost of 3D CNN grows cubically, but many voxels are empty due to point sparsity.  
Optimizations such as the sparse convolution [19] reduce the computation cost.  
Converting a point cloud to a 2D/3D grid suffers from the mismatch between the irregular distribution of points and the regular structure of the grids.

> 최근의 많은 연구는 point cloud를 regular grid로 변환하여 onvolutional neural networks을 활용한다.  
[20] point cloud를 2D BEV(Bird's Eye View) image에 투영하고 object detection을 위해 2D CNN을 사용한다.  
[4]은 BEV image와 FV(Front View) image에 point cloud을 투영한 후 두 image 모두에 2D CNN을 적용한다.  
이러한 투영은 제한된 image resolution로 인해 quantization error를 유도한다.  
일부 접근 방식은 point cloud를 3D 좌표로 유지한다.  
[23]은 3D voxels의 점을 나타내며 object detection를 위해 3D convolution을 적용한다.  
voxels의 해상도가 증가하면 3D CNN의 계산 비용은 입체적으로 증가하지만 많은 voxels은 point sparsity으로 인해 비어 있다.  
sparse convolution과 같은 Optimizations[19]는 계산 비용을 절감한다.  
point cloud를 2D/3D grid로 변환하면 points의 불규칙한 분포(irregular distribution)와 grids의 규칙적인 구조(regular structure) 사이의 불일치가 발생한다.  

**Point cloud in sets.**  
Deep learning techniques on sets such as PointNet [3] and DeepSet[22] show neural networks can extract features from an unordered set of points directly.  
In such a method, each point is processed by a multi-layer perceptron (MLP) to obtain a point feature vector.  
Those features are aggregated by an average or max pooling function to form a global feature vector of the whole set.  
[14] further proposes the hierarchical aggregation of point features, and generates local subsets of points by sampling around some key points.  
The features of those subsets are then again grouped into sets for further feature extraction.   
Many 3D object detection approaches take advantage of such neural networks to process a point cloud without mapping it to a grid.  
However, the sampling and grouping of points on a large scale lead to additional computational costs.  
Most object detection studies only use the neural network on sets as a part of the pipeline.   
[13] generates object proposals from camera images and uses [14] to separate points that belong to an object from the background and predict a bounding box.  
[16] uses [14] as a backbone network to generate bounding box proposals directly from a point cloud.  
Then, it uses a second-stage point network to refine the bounding boxes.  
Hybrid approaches such as [23] [19] [10] [21] use [3] to extract features from local point sets and place the features on a regular grid for the convolutional operation.  
Although they reduce the local irregularity of the point cloud to some degree, they still suffer the mismatch between a regular grid and the overall point cloud structure.

> PointNet [3] 및 DeepSet[22]와 같은 sets에 대한 딥 러닝 기술은 신경망이 unordered set of points에서 직접 features을 추출할 수 있음을 보여준다.  
이러한 방법에서 각 point은 point feature vector를 얻기 위해  multi-layer perceptron (MLP)에 의해 처리된다.  
이러한 features은 average 또는  max pooling function에 의해 집계되어 전체 집합의 global feature vector를 형성한다.  
[14] point features의 계층적 집계(hierarchical aggregation)를 추가로 제안하고, 일부 key points들을 sampling하여 points의 local subsets을 생성한다.  
그런 다음 subsets의 features은 추가 feature 추출을 위해 sets로 다시 grouping된다.  
많은 3D object detection 접근 방식은 이러한 신경망을 이용하여 grid에 매핑하지 않고 point cloud를 처리한다.  
그러나 large scale로 points을 sampling하고 grouping하면 추가 계산 비용이 발생한다.  
대부분의 object detection 연구는 sets에 신경망만 pipeline의 일부로 사용한다.  
[13]은 camera images에서 object proposals을 생성하고 [14]를 사용하여 개체에 속한 points을 배경에서 분리하고 bounding box를 예측한다.  
[16]은 [14]를 backbone network로 사용하여 point cloud에서 직접 bounding box proposals을 생성합니다.  
그런 다음, 2단계 point network를 사용하여 bounding boxes를 세분화한다.  
[23] [19] [10] [21]과 같은 Hybrid 접근 방식은 [3]을 사용하여 local point sets에서 features을 추출하고 convolutional operation을 위해 regular grid에 features을 배치한다.  
point cloud의 local irregularity은 어느 정도 감소하지만, 여전히 regular grid와 overall point cloud structure간의 불일치를 겪는다.  

**Point cloud in graphs.**  
Research on graph neural network [18] seeks to generalize the convolutional neural network to a graph representation.  
A GNN iteratively updates its vertex features by aggregating features along the edges.  
Although the aggregation scheme sometimes is similar to that in deep learning on sets, a GNN allows more complex features to be determined along the edges.  
It typically does not need to sample and group vertices repeatedly.  
In the computer vision domain, a few approaches represent the point cloud as a graph.  
[15] uses a recurrent GNN for the semantic segmentation on RGBD data.  
[9] partitions a point cloud to simple geometrical shapes and link them into a graph for semantic segmentation.  
[2] [17] look into classifying a point cloud using a GNN.  
So far, few investigations have looked into designing a graph neural network for object detection, where an explicit prediction of the object shape is required.

> graph neural network[18]에 대한 연구는 convolutional neural network을 graph representation으로 generalize하려고 한다.  
GNN은 edges(간선)를 따라 features을 집계하여 vertex(정점) features을 반복적으로 업데이트한다.  
aggregation scheme가 때때로 sets에 대한 deep learning에서와 유사하지만, GNN을 사용하면 edges를 따라 더 complex features을 결정할 수 있다.  
일반적으로 vertices을 반복적으로 샘플링하고 그룹화할 필요가 없다.  
computer vision 도메인에서 몇 가지 접근 방식은 point cloud를 graph로 나타낸다.  
[15]는 RGBD data의 semantic segmentation을 위해 반복 GNN을 사용한다.  
[9] point cloud을 단순한 geometrical shapes에 분할하고 이를 semantic segmentation을 위한 graph로 link한다.  
[2] [17] GNN을 사용하여 point cloud를 분류합니다.  
지금까지 object shape의 명시적 예측이 필요한 object detection를 위한 graph neural network 설계를 검토한 연구는 거의 없다.  

Our work differs from previous work by designing a GNN for object detection.  
Instead of converting a point cloud to a regular gird, such as an image or a voxel, we use a graph representation to preserve the irregularity of a point cloud.  
Unlike the techniques that sample and group the points into sets repeatedly, we construct the graph once.  
The proposed Point-GNN then extracts features of the point cloud by iteratively updating vertex features on the same graph.  
Our work is a single-stage detection method without the need to develop a second-stage refinement neural networks like those in [4][16][21][11][13].

> 우리의 작업은 object detection를 위한 GNN을 설계함으로써 이전 작업과 다르다.  
point cloud를 image 또는 voxel과 같은 regular gird로 변환하는 대신 graph representation을 사용하여 point cloud의 불규칙성(irregularity)을 보존한다.  
points을 반복적으로 샘플링하고 세트로 그룹화하는 기술과 달리, 우리는 graph를 한 번 구성한다.  
그런 다음 제안된 Point-GNN은 동일한 graph에 vertex features을 반복적으로 업데이트하여 point cloud의 features을 추출한다.  
우리의 연구는 [4][16][21][11][13]과 같은 second-stage refinement neural networks을 개발할 필요가 없는 single-stage detection method이다.  

## 3. Point-GNN for 3D Object Detection in a Point Cloud

In this section, we describe the proposed approach to detect 3D objects from a point cloud.  
As shown in Figure 2, the overall architecture of our method contains three components:  
(a) graph construction, (b) a GNN of T iterations, and (c) bounding box merging and scoring.

![Fig2](/assets/img/Blog/papers/PointGNN/Fig2.JPG)

## 3.1. Graph Construction
Formally, we define a point cloud of $$N$$ points as a set $$P = \{p_1, ... , p_N\}$$, where $$p_i = (x_i, s_i)$$ is a point with both 3D coordinates $$x_i \in R^3$$ and the state value $$s_i \in R^k$$ a k- length vector that represents the point property.  
The state value $$s_i$$ can be the reflected laser intensity or the features which encode the surrounding objects.  
Given a point cloud $$P$$, we construct a graph $$G = (P,E)$$ by using $$P$$ as the vertices and connecting a point to its neighbors within a fixed radius $$r$$, i.e.

$$E = \{(p_i, p_j) | \; ||x_i - x_j||_2 < r\} \qquad \qquad \qquad (1)$$

The construction of such a graph is the well-known fixed radius near-neighbors search problem.  
By using a cell list to find point pairs that are within a given cut-off distance, we can efficiently solve the problem with a runtime complexity of $$O(cN)$$ where $$c$$ is the max number of neighbors within the radius [1].

In practice, a point cloud commonly comprises tens of thousands of points. Constructing a graph with all the points as vertices imposes a substantial computational burden.  
Therefore, we use a voxel downsampled point cloud $$\hat{P}$$ for the graph construction. It must be noted that the voxels here are only used to reduce the density of a point cloud and they are not used as the representation of the point cloud.  
We still use a graph to present the downsampled point cloud.  

To preserve the information within the original point cloud, we encode the dense point cloud in the initial state value $$s_i$$ of the vertex.  
More specifically, we search the raw points within a $$r_0$$ radius of each vertex and use the neural network
on sets to extract their features.  
We follow [10] [23] and embed the lidar reflection intensity and the relative coordinates using an $$MLP$$ and then aggregate them by the Max function.  
We use the resulting features as the initial state value of the vertex.  
After the graph construction, we process the graph with a GNN, as shown in Figure 2b.

## 3.2. Graph Neural Network with AutoRegistration

A typical graph neural network refines the vertex features by aggregating features along the edges.  
In the $$(t+1)^{th}$$ iteration, it updates each vertex feature in the form:  

$$v^{t+1}_i = g^t(p(\{e_{ij}^t | (i, j) \in E\}), v^t_i )$$
$$e_{ij}^t= f^t(v^t_i , v^t_j)$$

where $$e^t$$ and $$v^t$$ are the edge and vertex features from the tth iteration.  
A function $$f^t(.)$$ computes the edge feature between two vertices.  
$$p(.)$$ is a set function which aggregates the edge features for each vertex.  
$$g^t(.)$$ takes the aggregated edge features to update the vertex features.  
The graph neural network then outputs the vertex features or repeats the process in the next iteration.

In the case of object detection, we design the GNN to refine a vertex’s state to include information about the object where the vertex belongs.  
Towards this goal, we re-write Equation (2) to refine a vertex’s state using its neighbors’
states:  
$$s^{t+1}_i = g^t(p(\{f^t(x_j - x_i, s^t_j) | (i, j) \in E\}), s^t_i ) \qquad \qquad \qquad (3)$$

Note that we use the relative coordinates of the neighbors as input to $$f^t(.)$$ for the edge feature extraction.  
The relative coordinates induce translation invariance against the global shift of the point cloud.  
However, it is still sensitive to translation within the neighborhood area.  
When a small translation is added to a vertex, the local structure of its neighbors remains similar.  
But the relative coordinates of the neighbors are all changed, which increases the input variance to $$f^t(.)$$.  
To reduce the translation variance, we propose aligning neighbors’ coordinates by their structural features instead of the center vertex coordinates.  
Because the center vertex already contains some structural features from the previous iteration, we can use it to predict an alignment offset, and propose an auto-registration mechanism:

$$\Delta x_i^t = h^t(s^t_i) \qquad \qquad \qquad (4)$$  
$$s^{t+1}_i = g^t(p({f(x_j - x_i + \Delta x_i^t, s^t_j)}, s^t_i)$$

$$\Delta x_i^t$$ is the coordination offset for the vertices to register their coordinates.  
 $$h^t(.)$$ calculates the offset using the center vertex state value from the previous iteration.  
By setting $$h^t(.)$$ to output zero, the GNN can disable the offset if necessary.  
In that case, the GNN returns to Equation (3).  
We analyze the effectiveness of this auto-registration mechanism in Section 4.

As shown in Figure 2b, we model $$f^t(.)$$, $$g^t(.)$$ and $$h^t(.)$$ using multi-layer perceptrons (MLP) and add a residual connection in $$g^t(.)$$.  
We choose $$p(.)$$ to be $$Max$$ for its robustness[3].  
A single iteration in the proposed graph network is then given by:

$$\Delta x_i^t = MLP^t_h(s^t_i)$$
$$e^t_{ij} = MLP^t_f([x_j - x_i + \Delta x_i^t, s^t_j])$$
$$s^{t+1}_i = MLP^t_g(Max(\{e_{ij} | (i, j) \in E\})) + s^t_i \qquad \qquad \qquad (5)$$
where [,] represents the concatenation operation.

Every iteration t uses a different set of $$MLP^t$$, which is not shared among iterations.  
After $$T$$ iterations of the graph neural network, we use the vertex state value to predict both the category and the bounding box of the object where the vertex belongs.  
A classification branch $$MLP_{cls}$$ computes a multi-class probability.  
Finally, a localization branch $$MLP_{loc}$$ computes a bounding box for each class.

## 3.3. Loss

For the object category, the classification branch computes a multi-class probability distribution $$(p_{c_1} , ... , p_{c_M})$$ for each vertex.  
$$M$$ is the total number of object classes, including the _Background_ class.  
If a vertex is within a bounding box of an object, we assign the object class to the vertex.
If a vertex is outside any bounding boxes, we assign the background class to it. We use the average cross-entropy loss as the classification loss.

$$l_{cls} = - \frac{1}{N} \sum_{i=1}^N \sum_{j=1}^N y^i_{c_j}log(p^i_{c_j}) \qquad \qquad \qquad (6)$$
where $$p^i_c$$ and $$y^i_c$$ are the predicted probability and the one-hot class label for the $$i$$-th vertex respectively.

For the object bounding box, we predict it in the 7 degree-of-freedom format $$b = (x, y, z, l, h, w, \mathit{\theta})$$, where $$(x, y, z)$$ represent the center position of the bounding box, $$(l,h,w)$$ represent the box length, height and width respectively, and  is the yaw angle.  
We encode the bounding box with the vertex coordinates $$(x_v, y_v, z_v)$$ as follows:

$$\delta_x = \frac{x-x_v}{l_m},\delta_y = \frac{y-y_v}{h_m},\delta_z = \frac{z-z_v}{w_m}$$
$$\delta_l =log(\frac{l}{l_m}) \delta_h = log(\frac{h}{h_m}) \delta_w = log(\frac{w}{w_m})$$
$$\delta_\theta = \frac{\theta - \theta_0}{\theta_m} \qquad \qquad \qquad (7)$$
where $$l_m, h_m, w_m, \theta_0, \theta_m$$ are constant scale factors.

The localization branch predicts the encoded bounding box $$\delta_b = (\delta_x, \delta_y, \delta_z, \delta_l, \delta_h, \delta_w, \delta_\theta)$$ for each class.  
If a vertex is within a bounding box, we compute the Huber loss [7] between the ground truth and our prediction.  
If a vertex is outside any bounding boxes or it belongs to a class that we do not need to localize, we set its localization loss as zero.  
We then average the localization loss of all the vertices:

$$l_{loc} = \frac{1}{N} \sum^X_{i=1} \mathbb{1}(v_i \in b_{interest}) \sum_{\delta \in \delta_{b_i}}l_{huber}(\delta - \delta^{gt})\qquad \qquad \qquad (8)$$

To prevent over-fitting, we add L1 regularization to each MLP.  
The total loss is then:
$$l_{total} =\alpha l_{cls} + \beta l_{loc} + \gamma l_{reg} \qquad \qquad \qquad (9)$$
where $$\alpha, \beta$$ and $$\gamma $$ are constant weights to balance each loss.

## 3.4. Box Merging and Scoring

As multiple vertices can be on the same object, the neural network can output multiple bounding boxes of the same object.  
It is necessary to merge these bounding boxes into one and also assign a confidence score.   
Non-maximum suppression (NMS) has been widely used for this purpose. The common practice is to select the box with the highest classification score and suppress the other overlapping boxes.  
However, the classification score does not always reflect the localization quality.   Notably, a partially occluded object can have a strong clue indicating the type of the object but lacks enough shape information. The standard NMS can pick an inaccurate bounding box base on the classification score alone.

![algorithm1](/assets/img/Blog/papers/PointGNN/alg1.JPG)

To improve the localization accuracy, we propose to calculate the merged box by considering the entire overlapped box cluster.  
More specifically, we consider the median position and size of the overlapped bounding boxes.  
We also compute the confidence score as the sum of the classification scores weighted by the Intersection-of-Union (IoU) factor and an occlusion factor.  
The occlusion factor represents the occupied volume ratio.  
Given a box $$b_i$$, let $$l_i, w_i, h_i$$ be its length, width and height, and let $$v^l_i, v^w_i , v^h_i$$ be the unit vectors that indicate their directions respectively.  
$$x_j$$ are the coordinates of point $$p_j$$. The occlusion factor $$o_i$$ is then:

$$o_i = \frac{1}{1_i w_i h_i} \prod_{v \in \{v^l_i,v^w_i,v^h_i\}} \max_{p_j \in b_i}(v^T x_j) - \min_{p_j \in b_i}(v^T x_j)$$

We modify standard NMS as shown in Algorithm 1.  
It returns the merged bounding boxes $$\mathcal{M}$$ and their confidence score $$\mathcal{Z}$$.  
We will study its effectiveness in Section 4.

## 4. Experiments

### 4.1. Dataset
We evaluate our design using the widely used KITTI object detection benchmark [6].  
The KITTI dataset contains 7481 training samples and 7518 testing samples.  
Each sample provides both the point cloud and the camera image.  
We only use the point cloud in our approach.  
Since the dataset only annotates objects that are visible within the image, we process the point cloud only within the field of view of the image.  
The KITTI benchmark evaluates the average precision (AP) of three types of objects: Car, Pedestrian and Cyclist.  
Due to the scale difference, we follow the common practice [10] [23] [19] [21] and train one network for the Car and another network for the Pedestrian and Cyclist.  
For training, we remove samples that do not contain objects of interest.

### 4.2. Implementation Details

### 4.3. Data Augmentation

### 4.3.1 Results

We have submitted our results to the KITTI 3D object detection benchmark and the Bird’s Eye View (BEV) object detection benchmark.  
In Table 1 and Table 2, we compare our results with the existing literature.  
The KITTI dataset evaluates the Average Precision (AP) on three difficulty levels:
Easy, Moderate, and Hard. Our approach achieves the leading results on the Car detection of Easy and Moderate level and also the Cyclist detection of Moderate and Hard level.   Remarkably, on the Easy level BEV Car detection, we surpass the previous state-of-the-art approach by 3.45.  
Also, we outperform fusion-based algorithms in all categories except for Pedestrian detection. In Figure 3, we provide qualitative detection results on all categories.  
The results on both the camera image and the point cloud can be visualized.  
It must be noted that our approach uses only the point cloud data.  
The camera images are purely used for visual inspection since the test dataset does not provide ground truth labels.  
As shown in Figure 3, our approach still detects Pedestrian reasonably well despite not achieving the top score.  
One likely reason why Pedestrian detection is not as good as that for Car and Cyclist is that the vertices are not dense enough to achieve more accurate bounding boxes.

![Table1](/assets/img/Blog/papers/PointGNN/Table1.JPG)

### 4.4. Ablation Study

![Fig3](/assets/img/Blog/papers/PointGNN/Fig3.JPG)

![Table3](/assets/img/Blog/papers/PointGNN/Table3.JPG)

![Fig4](/assets/img/Blog/papers/PointGNN/Fig4.JPG)

![Table4](/assets/img/Blog/papers/PointGNN/Table4.JPG)

![Table5](/assets/img/Blog/papers/PointGNN/Table5.JPG)

## 5. Conclusion

We have presented a graph neural network, named Point-GNN, to detect 3D objects from a graph representation of the point cloud.  
By using a graph representation, we encode the point cloud compactly without mapping to a grid or sampling and grouping repeatedly.  
Our Point-GNN achieves the leading accuracy in both the 3D and Bird’s Eye View object detection of the KITTI benchmark.  
Our experiments show the proposed auto-registration mechanism reduces transition variance, and the box merging and scoring operation improves the detection accuracy.  
In the future, we plan to optimize the inference speed and also fuse the inputs from other sensors.
