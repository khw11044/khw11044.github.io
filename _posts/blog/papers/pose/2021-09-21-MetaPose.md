---
layout: post
bigtitle:  "MetaPose: Fast 3D Pose from Multiple Views without 3D Supervision"
subtitle:   "번역"
categories:
    - blog
    - papers
    - pose-estimation
tags:
    - pose
comments: true
published: true
---

 2021 CVPR [paper](https://arxiv.org/pdf/2108.04869.pdf)

* toc
{:toc}


# MetaPose: Fast 3D Pose from Multiple Views without 3D Supervision

## Abstract  

 We provide an extensive ablation study separating the error due to the camera model, number of cameras, initialization, and image-space joint localization from the additional error introduced by our model.

최근 알려진 camera parameters를 사용한 monocular 및 multi-view pose estimation에서 큰 발전이 이루어진 반면, 위치와 방향을 알 수 없는 multiple cameras의 pose estimation은 훨씬 덜 주목을 받았다.  
본 논문에서, 우리는 정확한 3D pose와 camera estimation을 수행할 수 있고, multiple views에서 occlusion으로 인한 joint location 불확실성을 고려하며, training을 위해 2D keypoint data만 필요한 neural model을 train하는 방법을 보여준다.  
우리의 method는 잘 확립된 Human3.6M 뿐만 아니라 더 challenging in-the-wild한 움직이는 cameras상태인 SkiPose-PTZ dataset에서 classical bundle adjustment와 weakly-supervised monocular 3D baselines을 모두 능가한다.  
우리는 camera model, 카메라 수, 초기화 및 image-space joint localization으로 인한 오류를 우리의 model에 의해 도입된 additional error와 구분하는 광범위한 ablation study를 제공한다.

## 1. Introduction

본 논문에서, 우리는 supervision을 위해 captured images에서 이러한 joints의 2D positions만 사용하여 unknown positions, orientations 및 intrinsic parameters와 synchronized (potentially moving) cameras를 사용하여 captured한 RGB images에서 human joints의 3D coordinates를 estimating하는 문제를 다룬다.

역사적으로 human 3D pose의 real-time capture는 고가의 전문 motion capture 장비를 구입할 수 있는 대기업에서만 수행되었다[7].  
원칙적으로, human body joints의 공간 좌표(spatial coordinates)는 이러한 카메라의 모든 intrinsic 및 extrinsic parameters(줄여서 "camera calibration"이라 함)를 사용할 수 있는 경우, 카메라 공간 관측(camera-space observations)에서 직접 triangulated가 가능하다[14, 19].

고정된 카메라에서 통제된 환경에서 또는 cross-view correspondences로부터 self-calibration이 가능한 short-baseline stereo captures에서 정확한 calibration을 획득하는 기술은 이전 연구에서 광범위하게 연구되었다.

앞서 언급한 제약 조건이 적용되지 않는 한 가지 시나리오는 스포츠 캡처로, 멀리 떨어진 이동 카메라를 사용하여 저혼합물 배경(low-texture backgrounds) 앞에서 극한 포즈를 취한 선수의 클로즈업이 포착되어 시야를 가로질러, feature correspondences이 거의 detect되지 않기 때문에 calibration이 불가능하다.

앞서 언급한 제약 조건이 적용되지 않는 한 가지 시나리오는 스포츠 캡처로, feature correspondences가 views에 걸쳐 거의 detected되지않기 때문에, far spaced moving cameras plain backgrounds preclude calibration를 사용하여 low-texture backgrounds 앞에 extreme poses를 취한 players의 closeups이 captured 된다.(즉,Figure 1)

known camera calibration 없이, 정확한 3D pose estimation에 중요한 서로 다른 views의 position과 uncertainty predictions을 연관시킬 수 있는 실질적인 방법이 없지만 대부분의 경우 joints의 subset은 (self-) occlusion으로 인해 single view로부부터 정확하게 위치시킬수 있다

Without known camera calibration, though, we have no practical way of relating position and uncertainty predictions from different views, which is critical for accurate 3D pose estimation, since, in most cases, only a subset of joints can be accurately localized from a single view due to (self-) occlusion.

그러나 알려진 카메라 보정 없이는 정확한 3D 포즈 추정에 중요한 서로 다른 관점의 위치와 불확실성 예측을 연관시킬 수 있는 실질적인 방법이 없다. 대부분의 경우 (자체) 폐색으로 인해 단일 관점의 일부만 정확하게 국소화할 수 있기 때문이다.

위의 모든 사항을 고려할 때, practical solution은 multiple views에서 location와 uncertainty 추정치를 집계(aggregate)할 수 있어야 하지만 엄청나게 expensive한 3D joint 또는 camera supervision에 의존해서는 안 된다. 아래에서 논의한 바와 같이 기존 approaches 중 어느 것도 이러한 요건을 완전히 만족시키지 못한다 (see the top half of Figure 1 for illustrations)

***Fully-supervised 3D pose estimation*** approaches 는 lowest estimation error를 발생시키지만, training [37] 또는 training과 inference [14]동안 known 3D camera specification(사양)을 사용한다.  
그러나 3D joint annotation과 full camera calibration (with moving cameras) in-the-wild의 비용이 엄청나게 높기 때문에 특정 환경을 대표하는 충분히 많은 양의 labeled datasets를 획득하기가 어려워지고 [31, 17], 따라서 rendering supervised methods을 적용할 수 없게 된다.


***Weakly-supervised monocular 3D*** methods, [13, 22, 36]과 2D-to-3D lifting networks [4, 35]는 data constraint을 완화하여 train 시간에 calibration 없이 multi-view 2D data만 사용하여 3D pose inference을 가능하게 한다.  
불행히도 inference 시, 이러한 methods는 한 번에 하나의 view에만 적용될 수 있으므로 cross-view 정보와 uncertainty을 활용할 수 없다.  
inference 시간에 monocular predictions의 rigid alignment를 수행하는 이러한 방법의 직접적인 확장은 Section 5에 나와 있는 것과 같이 sub-optimal estimation error를 산출한다.


3D pose estimation에 대한 ***Classical structure-from-motion*** approaches은 noisy 2D observations에서 camera와 3D pose 모두를 반복적으로 refine한다.  
그러나 이러한 methods는 ,inference 중에 몇 가지 최적화 단계를 수행해야 하기 때문에, 신경 대응 방법(neural counterparts)보다 느린 경우가 많으며, 더 중요한 것은 inference 속도를 높이기 위해 불확실성 추정(uncertainty estimates)과 유도 편향(inductive biase)을 고려하지 않아 ,특히 적은 카메라의 경우, 성능이 저조하고 소음에 대한 민감도가 높다는 것이다, as we show in Section 5.

이러한 한계를 극복하기 위해, "MetaPose"를 제안한다(Figure 1 참조).  
우리의 method for 3D pose estimation는 multiple views로부터 pose predictions과 uncertainty estimates를 집계(aggregates)하고, train과 inference 모두에서 3D joint annotations 또는 camera parameters가 필요없으며 resulting pipeline에 매우 적은 latency를 추가한다.
우리의 approach는 off-the-shelf weakly-supervised 3D network를 사용하여 pose와 camera setup에 대한 초기 추측을 형성하고 off-the-shelf 2D pose estimation network에 의해 생성된 2D joint location probability heatmaps을 사용하여 이 추측을 반복적으로 수정(refines)한다.  
inference를 가속화하고 off-the-shelf networks의 errors를 보완하기 위해, slow iterative optimization refinement을 모방하도록 neural optimizer를 훈련시킨다.  
이 modular approach은 low latency와 함께 low estimation error를 발생시킬 뿐만 아니라 필요에 따라 individual components의 performance를 쉽게 분석하고 new models, priors 또는 losses을 plug-in 할 수 있도록 한다. 우리의 주요 공헌은 다음과 같이 요약할 수 있다.

1. To the best of our knowledge, we are the first to show that a general-purpose feed-forward neural network can accurately estimate the 3D human pose and the camera configuration from multiple views, taking into account joint occlusions and prediction uncertainties.
우리가 아는한, general-purpose feed-forward neural network는 joint occlusion과 prediction uncertainties을 고려하여 multiple views에서 정확하게 3D human pose와 camera configuration을 estimate할수 있음을 보여준다.

2. We propose a 3-stage modular training pipeline that uses only 2D supervision at train time, and a fast 2-stage inference pipeline. 훈련시에는 오로지 2D supervision만 사용하는 3-stage modular training pipeline을 제안하고 fast 2-stage pipeline을 제안한다.

3. On the well-established Human3.6M [12] dataset our method yields the lowest pose estimation error across models that do not use 3D ground truth annotations (more than 40% improvement in PMPJPE over the SotA) when using all four cameras and degrades gracefully when restricted to just two cameras (more than 40% improvement over the best baseline).  
잘 확립된 human3.6m dataset에서 우리의 method는 4개의 camera를 모두 사용할때, 3D ground truth annotations를 사용하지 않는 models들중에서 lowest pose estimation error를 산출(SOTA보다 PMPJPE면에서 40% 향상)하고 두개의 카메라로 제한되었을때는 단아하게 떨어진다.(best baseline보다 40%향상)

4. On the challenging in-the-wild SkiPose-PTZ [31] dataset with moving cameras, our method yields error comparable to the bundle adjustment baseline [19] when using all six cameras, but has much faster inference time, and massively outperforms bundle adjustment when restricted to just two cameras.
움직이는 카메라가 있는 challenging in-the-wild한 SkiPose-PTZ dataset에서 6개 카메라 모두 사용했을 때, 우리의 method는 bundle adjustment baseline [19]과 비교할만한 error를 산출하지만 inference 시에는 더 빠르고 두개의 카메라로 제한되있을때 bundle adjustment를 상당히 능가한다.

5. We conduct a detailed ablation study dissecting sources of error into the error due to the weak camera model, imperfect initialization, imperfect 2D heatmaps, and the additional error introduced by the neural optimizer, suggesting areas where further progress can be made.
weak camera model, 불완전 초기화, 불완전 2D heatmaps 그리고 neural optimizer에 의해 도입된 additional error로 인해 sources of error를 자세히 해부하여 detailed ablation study를 수행한다


## 2. Related Work

## 3. Method

1. We first acquire single-view estimates of the full 3D pose using a pre-trained monocular 3D network (EpipolarPose [22] for H36M and CanonPose [36] for SkiPose-PTZ) and infer an initial guess for the camera configuration by applying closed-form rigid alignment to these single-view 3D pose estimates.

> 우리는 먼저 pre-trained monocular 3D network (EpipolarPose for H36M and CanonPose for SkiPose-PTZ)를 이용해서 전체 3D pose의 single-view estimates를 요구하고 이러한 single-view 3D pose estimates에 닫힌형식의(closed-form) rigid alignment를 적용하여 camera configuration에 대한 initial guess를 infer한다.

2. We compute detailed single-view heatmaps using a state-of-the-art monocular 2D network (Stacked Hourglass [29] for both datasets) pre-trained on available 2D labels and refine our initial guess for the 3D pose and cameras by maximizing the likelihood of re-projected points given these heatmaps via gradient descent.

> 우리는 2D lables가 사용가능한 pre-trained된 state-of-the-art monocular 2D network (Stacked Hourglass [29] for both datasets)를 이용하여 detailed single-view heatmap을 계산하고 gradient descent(경사 하강법)을 통해 이러한 heatmaps이 주어진 re-projected points의 likelihood를 maximizing(극대화)하여 3D pose와 cameras에 대해 우리의 initial guess를 refine(구체화) 한다.

3. We approximate this iterative refinement given an initial guess and multi-view heatmaps via a forward pass of a neural network

> 우리는 neural network의 forward pass를 통해 initial guess와 multi-view heatmap를 반복적으로 refinement하여 근사화한다.

This modular multi-stage approach lets us prime gradient descent in stage 2 with a “good enough” initialization to start from the correct basin of the highly non-convex multiview heatmap likelihood objective. Moreover, it lets us swap pre-trained modules for monocular 2D and 3D without re-training the entire pipeline whenever a better approach becomes available. Finally, the neural optimizer in stage 3 provides orders of magnitude faster inference than the iterative refinement.

> 이 modular multi-stage approach은 매우 non-convex multiview heatmap likelihood objective의 정확한 basin(분지,대야)에서 시작하기에 "충분히 좋은" 초기화로 stage 2에서 주요 경사 하강법을 가능하게 한다. 또한, 더 나은 접근 방식을 사용할 수 있을 때마다 전체 pipeline을 re-training하지 않고 pre-trained modules을 monocular 2D 및 3D로 swap(교환)할 수 있다. 마지막으로, stage 3의 neural optimizer는 반복적인 refinement보다 훨씬 빠른 추론을 제공한다.

***Setup***
우리는 training동안 ground truth 3D poses와 Camera Parameters $$\{y^{(t)}_{gt}\}^T_{t=0}$$에 접근할 수 없다.  
하지만 우리는 reprojection error 측면에서 새로운 multi-view image set에서 3D pose estimation $$p \in \mathbb{R}^{K \times 3}$$과 camera parameters의 vectors $$e^{(c)}$$에 대해 good guess를 산출하기 위해서 우리는 function $$f : (\{x^{(c)}\}^C_{c=0}) \mapsto y_{pred}$$ 를 learn하기를 원한다.

결국,우리는 ground truth 3D pose $$p^{(t)}_{gt}$$와 예측값 $$p^{(t)}_{pred}$$ 사이의 mean per-joint position error가 ground truth에 접근하지 않고 가능한 한 작기를 원하므로 method는 “weakly-supervised”이다. 다음에 나오는 내용에서는 가독성을 향상시키기 위해 단일 항목에 대해 이야기할 때 index $$t$$를 생략하기도 한다. 우리의 표기법은 보충의 Table 5에도 요약되어 있다.

***Prerequisites***
우리는 2D supervision을 훈련될수있는 2개의 "external" networks에 접근한다고 가정한다 :  
1) **monocular 2D pose model $$f_{2D}$$** : per-joint heatmaps $$f_{2D} : x \mapsto h \in \mathbb{R}^{h \times w \times K}$$를 산출한다.  
2) **self-supervised monocular 3D model $$f_{M3D}$$** :
camera-frame 3D pose estimates $$f_{M3D} : x \mapsto \tilde{p} \in \mathbb{R}^{K \times 3}$$ 를 산출한다, such that 각 joint $$\tilde{p}_{[k,(0,1)]}$$의 처음 2개 coordinates는 image에서 joint의 estimated한 pixel coordinates를 포함하고 마지막 coordinate $$\tilde{p}_{[k,2]}$$ 는 "same" units에 estimated joint depth를 포함한다.(예, all points간의 거리는 scale단위로 정확하다)  

우리는 각 heatmap을 2-dimensional _d_-component Gaussian mixture model $$g=\{(\mu_r,\sigma_r,w_r)\}^d_{r=0}$$로서 근사해가고 저장한다, $$\mu$$ : mean, $$\sigma$$ : spherical covariances, $$w$$ : weights.

이 “compressed(압축)” format은 두개의 main goals를 제공한다 :  
 예를 들어, pixel-level heatmap interpolation(보간)과 비교하여 joint position likelihood를 최적화(optimize)하기 쉬운 smooth function으로 바꾸고, neural optimizer에 대한 input의 치수성(dimensionality)을 감소시켜 더 작은 모델을 훈련시킬 수 있게 한다.

우리는 weak-projection camera model을 사용하고, 각 camera가 rotation matrix, pixel shift 그리고 scale parmaenters 의 tuple $$e^{(c)} = (R^{(c)}, t^{(c)}, s^{(s)})$$로 정의되고, projection operator는 $$\pi_k(p,(R,t,s)) = s \cdot [R \cdot p_k]_{(0,1)} + t$$로 정의된다.

### 3.1 Initial estimate - Stage 1.
this stage의 목적은 monocular 3D pose model $$f_{M3D}$$을 이용하여 initial guess $$y_{init} = (p_{init}, \{e^{(c)}_{init}\}^C_{c=0})$$를 얻는것이다.  
우리는 initial guess에 대한 정확한 표현을 제공하고 그 이면에 있는 직관적인 idea를 아래에 설명한다.  
우리는 first camera를 canonical frame으로 선택(예,$$R^{(0)}_{init}=I, t^{(0)}=\tilde{0}, s^{(0)}=1$$)하고, camera c에 대한 3D monocular prediction $$\tilde{p}^{(c)}$$부터 첫번째 camera $$\tilde{p}^{(0)}$$에 대한 monocular 3D estimate와 관련된 rotation $$R^{(c)}$$를 찾기위해 orthogonal Procrustes alignment (via SVD of the outer product of mean-centered poses)를 사용한다.  
마찬가지로, 우리는 다른 C - 1 cameras에 대해 $$\tilde{p}^{(0)}$$와 rotated, scaled 및 shifted $$\tilde{p}^{(c)}$$ 사이의 불일치(discrepancy)를 최소화하는 최적의 shifts(optimal) 척도를 찾을 수 있다.   
그러면 3D pose $$p_{init}$$에 대한 initial guess는 해당 optimal rotations, scales 그리고 shifts에 의해 첫번째 camera frame에 rigidly aligned된 모든 cameras의 monocular 3D pose predictions의 average이다.

The initial guess for the 3D pose P then is the average of monocular 3D pose predictions from all cameras rigidly aligned to the first camera frame by corresponding optimal rotations, scales and shifts.
그러면 3D 포즈 P에 대한 초기 추측은 해당 최적의 회전, 스케일 및 시프트에 의해 첫 번째 카메라 프레임에 견고하게 정렬된 모든 카메라의 단안 3D 포즈 예측의 평균이다.

### 3.2 Iterative refinement – Stage 2.

두번째 stage에서, 우리는 initial estimate $$y_{init}$$에서 시작하여 refined guess $$y_{ref}$$를 얻기위해 $$g^{(c)}_k = \{(\mu^{(c)}_{kr},\sigma^{(c)}_{kr}, w^{(c)}_{kr})\}$$인 Gaussian mixtures의 form, monocular heatmaps을 사용한다.

refinement optimization problem over compressed heatmaps은 다음과 같다 :

$$y_{ref} = (p_{ref},\{e^{(c)}_{ref}\}) = \arg \min_{y'}\mathcal{L}_{ref}(y',g)$$
$$\mathcal{L}_{ref}((p,\{e^{(c)}\}),g) = - \frac{1}{CK} \sum^C_c\sum^K_k \mathcal{L}_{logp}(\pi_k(p,e^{(0)}),g^{c}_k)$$  
$$\mathcal{L}_{logp}(q, (\mu, \sigma, w)) = LSE_r [\log_\epsilon(\frac{w_r}{2\pi\sigma^2_r}) - \frac{\Vert q- \mu_r \Vert^2}{2\sigma^2_r}]$$

$$LSE_r(x)$$ 와 $$\log_\epsilon$$은 $$\log(\sum_r \exp(x_r))$$과 $$\log(x)$$의 수치상 안정적인 versions이고, $$\mathcal{L}_{logp}$$는 Gaussian mixture이 적용된 point의 numerically stable log-likelihood이다.  
optimization 동안, 우리는 scale(always positive)을 log-scale로 reparameterized 하고 On the Continuity of Rotation Representations in Neural Networks 논문의  rotation with a 6D vector

### 3.3. Neural optimizer – Stage 3.

마지막 stage에서, 우리는 pose와 cameras에 대한 current guess $$y_i$$에 optimal update d$$y_{i+1}$$를 predict하기 위해 optimizer를 neural network $$f_{\theta}$$로서 train한다. 논문 Deep feedback inverse problem solver처럼.
특별히, update d$$y_{i+1}$$은 heatmap mixture parameters $$g$$, current guess $$y_i$$, 각 camera에 current guess의 projections $$\{\pi^{(c)}(y_i)\}^C_{c=0}$$, 그리고 refinement loss의 current value로부터 계산된다. (we omit the dependency of $$y_i$$ on $$\theta$$ in the first line for readability):

$$\text{d}y_{i+1}(\theta) = f_{\theta}(g,y_i,\{\pi^{(c)}(y_i)\}, \mathcal{L}_{ref}(y_i,g))$$,  
$$y_0 = y_{init}$$,  
$$y_{i+1}(\theta) = y_i(\theta) + \text{d}y_{i+1}(\theta)$$,   
$$y_{neur}=y_{M}(\theta)$$

전반적인 pose와 camera estimate가 iterative refinement(반복적인 개선)를 통해 얻은 pose와 camera와 근접하게 유지되도록한면서, neural network는 re-projected된 neural estimate $$y_{neur}$$과 ground truth 2D joint location $$q$$ 사이의 re-projection loss $$L_{\pi}$$를 최소화하며 최적화된다.

$$\theta^{*} = \arg \min_\theta \sum^T_t \mathcal{L}_{neur}(y^{(t)}_{neur}(\theta),y^{(t)}_{ref}, q^{t})$$
$$\mathcal{L}_{neur}(y_{neur},y_{ref},q) = \lambda_\pi \cdot \mathcal{L}_\pi (y_{neur},q) + \lambda_y \cdot \mathcal{L}_y(y_{neur},y_{ref})$$

reprojection loss $$\mathcal{L}_\pi$$ 는 다음과 같이 정의된다.

$$\mathcal{L}_\pi(y,q) = \sum_c \Vert \pi^{(c)}(y) - q^{(c)} \Vert^2_2$$

그리고 teacher loss $$\mathcal{L}_y(y_{neur}, y_{ref})$$  
는 (Stage 2에 iterative refinement에의해 주어진 camera와 pose에 대한 estimate)$$y_{ref}$$ 와 neural estimate $$y_{neur}$$ 사이의 distance를 측정한다:

$$\mathcal{L}_y((p_{neur},\{R^{(c)}_{neur},t^{(c)}_{neur}, s^{(c)}_{neur}\}),(p_{ref},\{R^{(c)}_{ref},t^{(c)}_{ref}, s^{(c)}_{ref}\})) \\
= \lambda_p \cdot \Vert p_{neur} - p_{ref} \Vert^2_2 + \lambda_t \cdot \sum \Vert t^{(c)}_{neur} - t^{(c)}_{ref} \Vert^2_2 \\ + \lambda_R \cdot \sum \Vert (R^{(c)}_{neur})^TR^{(c)}_{ref} - I \Vert^2_2 \\ + \lambda_s \cdot \sum \Vert \log(s^{c}_{neur}) - \log(s^{(c)}_{ref}) \Vert^2_2$$

Huynh은 L2-distance보다 direct optimization이 더 적합하다고 제안하기 때문에 우리는 rotations간의 distance를 사용한다. $$\mathcal{L}_y$$가 correct solution의 neighborhood를 향해 neural optimizer를 draw하고 $$\mathcal{L}_\pi$$가 큰 reprojection errors를 결과로내는 $$y_{ref}$$로부터 small (in terms of $$\mathcal{L}_y$$) deviations에 penalizes하기 때문에 우리는 두 losses가 다 필요하다.

**Plug-in Priors**
Stages 2와 3은 human poses에 대한 priors으로 쉽게 확장 될수 있다.  
예를들어, 우리는 specific individual의 normalized된 bone length ration $$b$$의 vectors [33]에 우리의 model을 conditioning함으로써 우리의 predictor를 더욱 "personalized"하게 만들 수 있다. 더 구체적으로, 우리는 limb length component를 refinement loss에 추가했다:  
$$\mathcal{L}^{bone}_{ref}((p,\{e\}),g,b) = \mathcal{L}_{ref}((p, \{e\}),g) + \lambda_b \cdot \mathcal{L}_{bone}(b,p)$$  
$$\mathcal{L}_{bone}(b,p) = \begin{Vmatrix} b - \frac{\text{BoneLength(p)}}{||\text{BoneLength(p)}||^2} \end{Vmatrix}_2$$

$$b$$가 각 optimizer step $$f_\theta$$에 input으로 통과하고 neural optimizer는 $$\mathcal{L}_{neur} + \lambda_{b} \cdot \mathcal{L}_{bone}$$을 최소화하는 방향으로 학습된다.

**Inference**  
inference동안, initial guess $$y_{init}$$을 얻기위해
rigid alignment over monocular 3D estimates (Stage 1)를 수행할 필요가 있다, 그리고 a feed-forward manner (Stage 3)에서 neural optimizer $$f_\theta$$를 this guess에 적용한다, iterative refinement (Stage 2)는 필수적이지 않다. 우리는 Supp.8.1에 training과 inference pseudo code를 제공한다.

## 4. Experiments

이번 section에서, 우리는 datasets와 mettics를
우리는 제안된  method의 성능을 검증하기 위해 사용한  datasets와  metrics를 지정하고, 각 stage와 각 supervision signal에 의해 제공되는  error의 개선(improvement)을 평가하기 위해 수행한 일련의 baselines 및 ablation experiments을 지정한다.

**Data**
 우리는 고정된 4개의 cameras로 찍은 Human3.6M(H36M) dataset과 더 challenging한 6개의 움직이는 pan-tilt-zoom cameras로 찍은 SkiPose-PTZ(SkiPose) dataset으로 우리의 method를 평가한다.  
 우리는 H36M의 standard train-test evaluated protocol인 subject 1,5,6,7,8은 training을 사용하고 9,11은 testing으로 사용한다.  
 추가적으로 우리는 H36M에서 각 16번째 frame을 사용하여, 결과적으로 24443 train, 8516 test example, 각 example은 4개의 cameras에 대한 정보를 포함한다.  
 우리는 우리의 method를 SkiPose의 2개의 subsets에 평가한다 : the “full” dataset used by Rhodin et al. [31] (1315 train / 284 test), 그리고 “clean” subset (1035 train / 230 test) used by Wandt et al. [36] 이것은 winter storm에 의해 시각적으로 상당히 가려진 views들은 배제하고 6개의 information을 포함한다.

우리는 small training set에 overfitting으로부터 neural optimizer를 막기위해 trianing동안 랜덤하게 camera 순서를 shuffling함으로서 SkiPose dataset을 augment한다.  
각 dataset에서, 우리는 train에서 첫 64 examples는 validation set으로 나눈다.

**Metrics**
Procrustes aligned Mean Per Joint Position Error (PMPJPE)은 optimal rigid alignment를 full predicted 3D pose와 ground truth 3D pose에 적용한 이후에 3D joint estimates의 L2-error를 측정한다, 그래서 전반적인 pose orientation error를 무시한다.  
 Normalized Mean Per Joint Position Error (NMPJPE)는 주어진 camera frame에 대한 prediction을 해당 camera frame의 groud truth pose로 optimally scaling과 shifting후에 error를 측정한다. 때문에 pose orientation에 민감하지만 "scale"에 대한 errors는 무시한다.  
 우리는 non-normalized pose estimation error는 보고하지않는다 왜냐하면 intinsic camera parameters가 없기 때문이다, correct absolute scale은 data로부터 estimated될수 없다.  

우리는 또한 각 view에 대한 monocular pose estimation time 외에 다른 methods을 사용하여 multi-view 3D pose inference을 수행하는 데 걸리는 additional time의 “order-of-magnitude”를 보고한다.

**Baselines**
H36M에서,

inference동안 multi-view predictions을 aggregate하기위해 ground truth camera parameters를 사용하는 Iskakov et al.[14]의 state-of-the-art fully-supervised baseline으로 error를 lower-bound한다.  
우리는 우리의 method의 performance와 training 동안 multi-view 2D supervision을 사용하지만 inference에서는 한 time당 single view를 사용하는 method : self-supervised
EpipolarPose (EP), CanonPose (CP), weakly supervised baselines of Iqbal et al.,Rhodin et al.를 비교한다.

우리는 H36M에 human bounding boxes의 우리 데이터에 EpipolarPose를 적용하므로 per-view는 Kocabas et al. [22]에 의해 reported 된 것들과 다른 결과를 가져온다.

Skipose에 대해서, 우리는 두개의 baseline만 우리의 method와 비교한다 : CanonPose와 Rhodin et al.
CanonPose는 fixed cameras를 가정하지 않으므로 CanonPose는 SkiPose dataset에 대해 EpipolarPose보다 선호된다.  
SkiPose의 subsets인 "clean"과 "full" 모두에 대한 결과를 포함한다; 그러나 CanonPose는 “clean” subset에 대해서만 trained되고 evaluated된다. 그래서 “full” SkiPose dataset에 대한 trained와 evaluated된 monocular 3D methods가 없기 때문에,  
완결성을 위해서, SkiPose의 subset "full"을 evaluating할때만 CanonPose prdictions 위치의 Gaussian noise를 주어 ground truth data를 사용해 우리의 method를 학습하고 평가하였다.

우리는 또한 우리의 method를 모든 cameras의 ground truth extrinsic camera parameters, 그리고 고정된 ground truth intrinsic parameters로 initialized된 "classical" bundle adjustment와 비교했, 그러므로 비현실적으로 favorable conditions(유리한 상황)에 놓이게 된다.
우리는 Zhou et al. [39]의 3D registration algorithm의 version을 채택한 AniPose [19]에서 well-tested implementation of bundle adjustment 을 사용하였다.  
이 approach는 joint locations의 point 추정치를 input으로 취하고(즉, uncertainty가 없음) camera parameters와 joint location estimates를 반복적으로 refine한다

**Ablation experiments.**

**Architecture**

## 5. Results

**Few-camera performance**

**Self-supervised performance**

**Analysis.**

### 5.1. Ablations

**Heatmap error**

**Camera model**

**Initialization**

**Design decisions**

## 6. Conclusions