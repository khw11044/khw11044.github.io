---
layout: post
bigtitle:  "Tacotron: Towards End-to-End Speech Synthesis"
subtitle:   "Tacotron, text-to-speech"
categories:
    - blog
    - papers
tags:
    - Tacotron
    - TextToSpeech
comments: true
published: true
---
# Tacotron: Towards End-to-End Speech Synthesis

## ABSTRACT(초록)

문자 음성 합성 시스템  
text-to-speech synthesis system 여러 단계로 구성 :

text analysis frontend

an acoustic model

an audio synthesis module

과 같은 multiple stages로 이루어져 있다.

이러한 구성들의 구축은 종종 광범위한 도메인 전문지식을 요구하며 아마 불안정한 설계 선택(brittle design choices)을 포함한다.

이 논문은, 직접적인 characters로 부터 speech를 합성(synthesize)하는 end-to-end generative text-to-speech model를 제시한다.

<text, audio> pairs를 고려하면, 이 model은 scratch with random initialization(랜덤 초기화된 scratch로 부터(?) 완벽하게 학습이 된다.

우리는 이 과업을 위해 sequence-to-sequence framework가 잘 수행되게 만드는 몇몇 핵심 기술(several key techniques)를 제시한다.

Tacotron은 US English에서 3.82 subjective 5-scale mean opinion score를 성취하며 naturalness에 관해 production parametric system을 능가한다.

또한, Tacotron은 frame 수준에서 speech를 생성하기 때문에, sample-level autoregressive methods(샘플수준의 자동 회귀 방법) 보다 훨씬 빠르다.

## 1 INTRODUCTION

현대의 text-to-text(TTS) 파이프라인은 복잡하다(Taylor, 2009).
예를 들어, 다양한 언어 특징들(various linguistic features), duration model, 음향 특징 예측 모델(acoustic feature prediction model) 그리고 복잡한 신호 처리 기반 vocoder를  추출하는 text frontend를 갖는것이 일반적이다(Zen et al., 2009; Agomyrgiannakis, 2015).

이러한 구성요소들은 광범위한 도메인 전문지식(extensive domain expertise)을 기반으로 하며 설계하는데 손이 많이 간다. 그것들은 또한 독립적으로 학습되기 때문에 각 요소별 오류들이 복합적으로 일어날수 있다.  
따라서 현대 TTS 설계의 복잡성은 새로운 시스템을 구축할때 상당한 엔지니어링 노력으로 이어진다.

따라서 minimal human annotations 으로 <text,audio>쌍에서 훈련할 수 있는 통합 end-to-end TTS 시스템의 많은 장점이 있다.  
첫째, 이러한 시스템은 heuristics and brittle design choices을 수반할 수 있는, 손이 많이 가는 특징(feature) 엔지니어링의 필요성을 완화시킨다.

둘째, 화자(speaker)나 언어와 같은 다양한 속성이나 감정(Sentiment)과 같은 고수준 특징들(features)에 대한 풍부한 조절을 더 쉽게 할 수 있다.  
이는 특정 구성 요소에만 국한 되지 않고 모델의 맨 처음에 조건화(conditioning)가 발생할수 있기 때문이다. 마찬가지로 새로운 데이터에 대한 적응도 더 쉬울 수 있다.

마지막으로, 단일 모델(single model)은 각 오류가 복합될수 있는 multi-stage model보다 더 강력(robust)할 수 있다. 이러한 장점은 end-to-end 모델이 실제 세계에서 발견되는 풍부하고 표현력이 뛰어난 데이터(하지만 종종 노이즈가 많은 데이터)를 훈련할 수 있음을 의미한다.


TTS는 large-scale inverse problem(: 매우 압축된(compressed) source(text)가 오디오로 decompress 되는)가 있다.  
동일한 텍스트라도 발음 또는 발화 스타일이 다르기 때문에, end-to-end 모델에서 특히 어려운 학습 과제이다. : 주어진 입력에 대한 signal level에서 큰 variations에 대처해야한다.

더욱이, end-to-end speech recognition(단대단 음성 인식)(Chan et al., 2016) 이나  machine translation (Wu et al., 2016)와 달리, TTS의 outputs는 연속적이고 output sequences는 일반적으로 input보다 훨씬 더 길다.
이러한 속성은 예측 오류를 빠르게 누적시킨다.

![tacotron1_01](/tacotron1_01.JPG)
> Figure 1: Model achitecture, 이 모델은 문자를 입력으로 받아들여 해당하는 raw spectrogram을 출력하고 그것을 Griffin-Lim reconstruction algorithm에 제공하여 음성을 합성한다.

본 논문에서 attention 패러다임(Bahdanau et al., 2014)을 가진 sequence-to-sequence (seq2seq) (Sutskever et al., 2014)를 기반으로 하는 end-to-end generative TTS model인 Tacotron을 제안한다.

우리의 모델은 characters를 input으로 raw spectrogram을 output으로 사용하며 vanilla seq2seq model의 기능을 향상시키기 위해 몇가지 기술을 사용한다.  
<text,audio> 쌍을 고려하여, Tacotron은 랜덤 초기화를 통해 처음부터 완전히 학습할 수 있다. phoneme-level alignment(음소 수준 정렬)이 필요로 하지 않으므로, 대량의 음성데이터(acoustic data)와 그에 맞는 레이블 텍스트(transcripts)를 이용하여 쉽게 scale 할수 있다.

간단한 파형 합성 기법(simple waveform synthesis technique)으로, Tacotron은 미국 영어 평가 세트에서 3.82 MOS(mean opinion score)를 얻어 naturalness면에서 parametric system 점수를 능가한다.

## 2 RELATED WORK

WaveNet (van den Oord et al., 2016)은 강력한 오디오 생성 모델이다. 이것은 TTS에서는 잘 작동하지만,  sample-level autoregressive nature 때문에 속도가 느리다. 또한 WaveNet은 기존 TTS frontend의 언어적 특징(linguistic features)에 대한 조절이 필요하므로 end-to-end가 아니며 이는 Vocoder와 음향 모델(acoustic model)만 대체한다.  
또한 최근에 대발된 DppeVoice(Arik et al., 2017)모델의 경우, 일반적인 TTS 파이프라인에서 모든 구성 요소가 해당하는 neural network로 대체한다. 그러나 각 구성요소는 독립적으로 학습되므로 시스템을 end-to-end 방식으로 바꾸는것은 쉬운게 아니다.

우리가 아는한, Wang et al. (2016)는 seq2seq와 attention을 이용한 end-to-end TTS를 다루는 초기 작업이다. 그러나 그것은 seq2seq 모델이 정렬을 학습하는 데 도움이 되는 사전에 훈련된 HMM(hidden Markov model) aligner가 필요하다.
얼마나 많은 alignment가 seq2seq에의해 학습되었는지 말하기 힘들다.  
둘째, 모델을 훈련시키기 위해 몇 가지 요령이 사용되며, 저자들은 운율(prosody)을 해친다고 지적한다.  
셋째, vocoder parameters를 예측한다. 따라서 vocoder가 필요하다. 게다가 이 모델은 음소(phoneme) inputs에 대해 훈현되며 실험 결과들은 다소 제한적인 것 같다.

Char2Wav (Sotelo et al., 2017)는 characters에 학습될수 있는 독립적으로 개발된 end-to-end 모델이다. 그러나 Char2Wav는 SampleRNN neural vocoder (Mehri et al., 2016)를 사용하기 전에 vocoder parameters를 예측해야하는 반면, Tacotron은 직접 raw spectrogram을 예측한다. 또한 이들의 seq2seq와 SampleRNN model들은 별도로 사전 학습이 필요하지만 Tacotron은 사전준비없이 학습이 이루어진다. 마지막으로, vanilla seq2seq 패러다임에서 몇몇 key를 수정했다.  
나중에 보여지듯이, vanilla seq2seq 모델은 character-level inputs에 대해 잘 작동하지 않는다.

## 3 MODEL ARCHITECTURE

Tacotron의 backbone은 seq2seq model with attention (Bahdanau et al., 2014; Vinyals et al.,2015)이다. Figure 1은 encoder, attention-based decoder, post-processing net을 포함하는 모델을 보여주고있다. high-level에서 Tacotron은 characters를 input으로 받고 waveforms(파형)으로 변환되는 spectrogram frames을 생성한다. 아래에는 이러한 구성 요소가 설명되어있다.

![tacotron1_02](/tacotron1_02.JPG)
> Figure 2: Lee et al. (2016)에 채택된 The CBHG (1-D convolution bank + highway network + bidirectional GRU) module


### 3.1 CBHG MODULE

먼저 Figure 2의 CBHG를 설명한다. CBHG은 1-D convolutional filter들의 bank, 그다음 hightway network(Srivastava et al., 2015) 와 bidirectional gated recurrent unit (GRU) (Chung et al., 2014) recurrent neural net (RNN)으로 구성되어있다. CBHG는 시퀀스에서 표현(representations)을 추출하는 강력한 모듈이다. 입력 시퀀스(input sequence)는 먼저 $K$ sets의 1-D 컨볼루션 필터로 컨볼루션되며, 여기서 k번째 세트는 k너비(width)의 $C_k$ 필터를 포함합니다$(k = 1, 2,...,K)$. 이러한 필터는 로컬 및 (unigrams, bigrams, up to K-grams 모델링과 유사한)contextual information를 명시적으로 모델링한다. 컨볼루션 출력이 함께 쌓이고 더욱이 local invariances를 증가시키기 위해서 오래동안 max pooling된다. 우리는 original time resolution를 지키디위해 stride 1을 사용했다. 처리된 시퀀스를 몇 개의 fixed-width 1-D convolutions에 추가로 전달하며, 출력은  residual connections (He et al., 2016)을 통해 original input sequence와 함께 추가된다. 배치 정규화(Iofe & Szegedy, 2015)는 모든 convolutional layer에 사용된다.
convolution outputs은 multi-layer highway network에 공급되어 high-level features를 추출한다.
마지막으로 bidirectional GRU RNN을 상단에 쌓아 forward와 backward context에 sequential features을 추출한다.
CBHG는 machine translation (Lee et al., 2016)작업에서 영감을 받았다. 여기서  Lee et al. (2016)와의 주요 차이점은 비확산 컨볼루션(non-causal convolutions)사용, batch normalization, residual connections 그리고 stride=1 max pooling이다.
우리는 이러한 수정이 일반화를 개선한다는 것을 발견했다.

### 3.2 ENCODER

Encoder의 목표는 텍스트의 robust sequential representations을 추출하는 것이다.
Encoder에 대한 input은 character sequence로 각각의 character는 one-hot vector로 표현되고 continuous vector로 임베딩된다. 그런다음 "pre-net"이라 불리는 non-linear transformations 집합을 각 임베딩에 적용한다. 이 작업에서 우리는 dropout과 함께 bottleneck layer를 pre-net으로 사용하여 수렴을 돕고 generalization을 개선한다.
CBHG module은 pre-net outputs을 attention module이 사용되는 최종 encoder representation으로 변환한다. CBHG기반 Encoder가 overfitting을 줄일뿐만 아니라 standard multi-layer RNN encoder보다 덜 잘못발음한다는것을 발견했다.

> Table 1: Hyper-parameters와 network architectures. “conv-k-c-ReLU”는 ReLu activation과 함께 width $k$와 $c$ output channels이 있는 1-D convolution을 의미한다.

![tacotron1_03](/tacotron1_03.JPG)

### 3.3 DECODER

우리는 content 기반의  tanh attention decoder(see e.g. Vinyals et al. (2015))를 사용한다. 여기서 stateful recurrent layer는 각 decoder time step에서 attention query를 생성한다. 우리는 context vector와 주의 attention RNN cell output을 연결(합)하여 decoder RNNs에 대한 input을 형성한다.
우리는 decoder에 a stack of GRUs with vertical residual connections (Wu et al., 2016)를 사용한다.
우리는 residual connections로 인해 수렴 속도가 빨라진다는 것을 발견했습니다.
decoder target은 중요한 design choice이다.
raw spectrogram을 직접 예측할 수 있지만 음성신호(speech signal)와 텍스트 사이의  alignment을 학습하기 위해서는 매우 불필요한(중복된)(redundancy) 표현이다(이 작업은 실제로 seq2seq를 사용하는 동기이다).
이러한 중복성(redundancy) 때문에, 우리는 seq2seq decoding 및 waveform synthesis(파형 합성)에 다른 대상을 사용한다.
seq2seq target은 수정되거나 훈련될수 있는 inversion process에 대한 prosody(운율체계) 정보와 sufficient intelligibility를 제공하는 한 매우 압축될수 있다.
우리는 80-band mel-scale spectrogram를 target으로 사용하지만, 더 적은 bands나 cepstrum과 같은 더 간결한(concise) target을 사용할 수 있다.
우리는 seq2seq target에서 waveform로 변환하기 위해서 post-processing network를 사용한다.


우리는 decoder targets를 예측하기 위해 simple fully-connected output layer 사용한다. 우리가 발견한 중요한 trick은 decoder step에서 multiple, non-overlapping output frames을 예측하는 것이다.
r frames을 한번에 예측은 총 decoder steps 수가 r로 분할되어 model size, 훈련 시간, 추론 시간이 단축된다.
더 중요한것은, 우리는 attention으로 학습한 더 빠르고 안정적인 alignment로 측정했을 때, 이 trick이 수렴 속도를 상당히 증가하였다는 것을 발견했다.
이것은 인접한 음성 프레임들(speech frames)이 상관관계가 있고 각 문자(character)가 일반적으로 여러 프레임에 해당되기 때문일 수 있습니다.
한번에 하나의 프레임을 방출(Emitting)하는것은 모델이 여러번의 단계에서 동일한 입력(input) 토큰에 참여하게 되어;여러 프레임을 방출(Emitting)은 attention이 훈련 초기에 forward로 이동하게 한다.
Zen et al. (2016)에서 유사한 trick이 사용되지만 주로 속도를 높이기 위해 사용된다.

첫번째 decoder 단계는 <GO> 프레임을 나타내는 all-zero frame에서 조절됩니다.
추론하면, decoder 스텝 t에서, r 예측의 마지막 프레임은 스텝 t+1에서 decoder에 대한 입력으로 공급된다.  
Note that feeding the last prediction is an ad-hoc choice here – we could use all r predictions.
훈련 동안, 우리는 항상 decoder에 모든 r번째 ground truth frame을 공급한다.
입력 프레임(input frame)은 encoder처럼 pre-net으로 전달된다.
스케쥴된 samplin (Bengio et al., 2015) (우리는 그것이 오디오 품질을 해친다는것을 발견)와 같은 기술들을 우리는 사용하지 않아기 때문에, pre-net에서 dropout은 모델을 일반화(generalize)하는데 중요하다. 왜냐하면 pre-net에서 dropout은 모델이 output distribution(출력분포)에서 multiple modalities를 해결하기 위해 noise source를 제공하기 때문이다.

### 3.4 POST-PROCESSING NET AND WAVEFORM SYNTHESIS

위에서 언급한 바와 같이, post-processing net의 task는 seq2seq target가 waveforms(파형)으로 합성할 수 있는 target으로 변환하는 것이다. 우리는 Griffin-Lim을 합성기(synthesizer)로 사용되기 때문에, post-processing net는 linear-frequency scale로 샘플링된 스펙트럼 크기(spectral magnitude)를 예측하는 것을 학습한다. post-processing net의 또 다른 동기는 완전히 decode된 sequence를 볼수 있다는 것이다. 항상 왼쪽에서 오른쪽으로 실행되는 seq2seq와 대조적으로, 그것은 각 개별 프레임에 대한 예측 오류를 바로잡기 위해 forward,backward 정보가 모두 있다. 이 작업에서, 더 단순한 아키텍처도 잘 작동할 수 있지만, 우리는 post-processing net를 위해 CBHG module을 사용한다. post-processing network의 개념은 매우 일반적이다. 그것은 vocoder parameters와 같은 alternative targets를 예측하거나 waveform 샘플을 직접 합성하는 WaveNet과 같은 neural vocoder (van den Oord et al., 2016; Mehri et al., 2016; Arik et al., 2017)로 사용할수 있다.

Griffin-Lim algorithm (Griffin & Lim, 1984)을 사용하여 예측된 spectrogram의 waveform을 합성한다.
We found that raising the predicted magnitudes by a power of 1.2 before feeding to Griffin-Lim reduces artifacts, likely due to its harmonic enhancement effect.
우리는 50회 iteration을 한 후에 Griffin-Lim이 합리적으로 빠르게 수렴하는 것을 관찰했다.(실제로 약 30회 반복하면 충분해 보인다.)
우리는 Griffin-Lim을 TensorFlow(Abadi et al., 2016)로 구현하였다. Griffin-Lim은 미분 가능하지만(훈련 가능한 가중치가 없다) 작업에서 우리는 그것에 대해 어떠한 손실도 부과하지 않는다. 우리가 Griffin-Lim을 선택하는 것은 단순하다; 이미 강력한 결과를 산출하였지만, waveform inverter에서 빠르고 고품질의 훈련 가능한 spectrogram을 개선하는 것은 진행 중이다.

## 4 MODEL DETAILS

Table 1에는 hyper-parameters와 network architectures가 나열되어 있습니다.
우리는 Hann windowing, 50 ms frame length, 12.5 ms frame shift, 2048-point Fourier transform(푸리에 변환)과 함께 log magnitude spectrogram을 사용한다.
우리는 또한 pre-emphasis (0.97)가 도움이 되는것을 발견했다. 우리는 모든 실험에 4 kHz sampling rate를 사용한다. 비록 더 큰 r값(e.g. r = 5)이 더 잘 작동하지만, 이논문에서는 MOS 결과를 위해  r = 2 (output layer reduction factor)를 사용한다.
우리는 Adam optimizer (Kingma & Ba, 2015)를 사용하고 learning rate decay는 각각 500K, 1M, 2M global steps이후, 0.001에서 시작하여 0.0005, 0.0003, 0.0001로 줄인다.
seq2seq decoder (mel-scale spectrogram)와 post-processing net (linear-scale spectrogram) 모두에 대해 simple $l$ 1 loss을 사용한다. 두 loss는 같은 가중치를 갖는다.
우리는 모든 sequence가 최대 길이로 패팅되는 32 batch size를 사용한다.
It’s a common practice to train sequence models with a loss mask, which masks loss on zero-padded frames.
하지만 우리는 이런 방식으로 학습된 모델이 언제 emitting outputs를 멈추는지를 몰라서 마지막에 소리를 반복하는것을 발견했다.
이 문제를 해결하기 위한 한 가지 간단한 방법은 zero-padded frames을 재구성하는 것이다.

## 5 EXPERIMENTS
전문 여성 화자가 말하는 약 24.6시간의 음성 데이터를 포함하는 북미 영어 데이터 세트로 Tacotron을 학습하였다.
phrases는 text normalize 된다. 예를 들어 "16"은 "sixteen"으로 변환된다.

![tacotron1_04](/tacotron1_04.JPG)
> Figure 3: Attention alignments on a test phrase. Tacotron에서 decoder 길이는 output reduction factor(출력감소계수) r=5를 사용하기 때문에 더 짧다.

### 5.1 ABLATION ANALYSIS
우리는 우리의 모델의 주요 구성 요소를 이해하기위해 몇가지 ablation 연구를 수행한다.
generative model에서 흔히 볼수 있듯이, 종종 perception(인식?)(Theis et al., 2015)과 잘 상관 없는 objective metrics(객관적인 메르릭스?)를 기반으로 모델을 비교하는것은 어렵다. 대신에 우리는 주로 시각적 비교에 의존한다. 우리는 강력하게 제공된 샘플들을 듣기를 권고한다.

![tacotron1_05](/tacotron1_05.JPG)
> Figure 4: post-processing net을 사용한 Predicted spectrograms와 사용하지 않은 Predicted spectrograms이다.

첫번째, 우리는 vanilla seq2seq model과 비교한다. encoder와 decoder 모두 두개의 residual RNNs layer을 사용하며 각 layer는 256 GRU cells(LSTM을 시도했고 유사한 결과를 얻었다)을 가지고 있다. pre-net 또는 post-processing net를 사용하지 않았으며, decoder는 linear-scale log magnitude spectrogram를 직접 예측한다.  
우리는 이 모델이  alignments을 학습하고 generalize(일반화)하려면 스케줄링된 sampling (sampling rate 0.5)이 필요하다는 것을 발견했다. 우리는 Figure 3 에서 학습된 attention alignment를 보여준다. Figure 3(a)는 vanilla seq2seq가 poor alignment을 학습한다는 것을 나타낸다. 한가지 문제는 attention이 moving forward전에 많은 프레임에 대해 고착되는 경향이 있어 synthesized signal(합성신호)에서 speech intelligibility이 저하되는 원인이 된다는 점이다. 그 결과 naturalness와 overall duration이 파괴된다. 대조적으로, 우리의 모델은 Figure 3(c)과 같이 깨끗하고 부드러운 alignment을 학습한다.

두번째, 우리는 2-layer residual GRU encoder로 대체된 CBHG encoder로 구성된 모델을 비교한다. encoder pre-net을 포함한 나머지 모델은 그대로 유지된다.
Figure 3(b)와 3(c)를 비교하면, 우리는 GRU encoder에서의 alignment가 noisier하드는것을 알 수 있다. synthesized signals(합성신호)를 들으면서, noisy alignment이 종종 잘못된 발음으로 이어진다는 것을 발견했다. CBHG encoder overfitting을 줄이고 길고 complex phrases(복잡한 구문)을 generalize한다.

Figures 4(a)와 4(b)는 post-processing net 사용의 장점을 보여준다. 우리는 다른 모든 구성 요소를 그대로 유지하면서 post-processing net 없는 모델을 훈련시켰다.(decoder RNN이 linear-scale spectrogram을 예측한다는 점을 제외)

더 많은 contextual information(상황별 정보)를 통해, post-processing net로부터의 예측은  더 잘 해결된 harmonics (e.g. higher harmonics between bins 100 and 400)와 synthesis artifacts를 감소기키는 high frequency formant structure를 포함한다.

### 5.2 MEAN OPINION SCORE TESTS

우리는 mean opinion score tests를 수행하는데, 피실험자들은 5-point Likert scale score로 stimuli의 naturalness을 평가하도록 요청받았다.
MOS 테스트는 원어민으로부터 크라우드소싱되었다.
테스트에는 보이지 않는 100개의 구문이 사용되었으며 각 구문은 8개의 등급을 받았다. MOS를 계산할 때는 헤드폰이 사용된 등급만 포함한다.
우리는 두 가지 모두 생산중인 parametric (based on LSTM (Zen et al., 2016))와 concatenative system (Gonzalvo et al., 2016)을 우리의 모델과 비교한다.
Table 2에서 볼수 있듯이, Tacotron은 3.82의 MOS를 달성한다. 이것은 parametric system을 능가한다. 강력한 baselines과 Griffin-Lim 합성(synthesis)에 의해 도입된 artifacts을 고려하면, 이것은 매우 유망한 결과를 나타낸다.

> Table 2: 5-scale mean opinion score evaluation

![tacotron1_06](/tacotron1_06.JPG)

## 6 DISCUSSIONS

character sequence를 입력으로 받아 해당 spectrogram을 출력하는 integrated end-to-end generative TTS model인 Tacotron을 제한했다.
매우 간단한 파형 합성 모듈(waveform synthesis module)을 통해 미국 영어에서 3.82 MOS 점수를 획득하여 naturalness 측면에서 production parametric system을 능가한다. Tacotron은 프레임 기반이므로 추론 속도가 sample-level autoregressive methods보다 상당히 빠르다. 이전 작업과 달리 Tacotron은 손handengineered linguistic features이나 HMM aligner와 같은 복잡한 구성 요소가 필요하지 않다. 랜덤 초기화를 통해 처음부터 학습할 수 있다. 학습된  text normalization의 최근 발전(Sproat & Jaitly, 2016)이 미래에 이것을 불필요하게 만들 수 있지만, 우리는 간단한  text normalization를 수행한다.

아직 모델의 많은 측면을 조사하지 않았다; 많은 초기 설계 결정은 변경되지 않았다.
우리의 output layer, attention module, loss function, waveform synthesizer기반의 Griffin-Lim 모두는 개선되었다. 예를 들어 Griffin-Lim outputs은 audible artifacts가 있을수 있다는 것은 잘 알려져 있다. 우리는 현재 spectrogram inversion 기반의 빠르고 고품질의 신경망에 대해 연구하고 있다.


## 요약
---
기존 모델들에 비해 End-To_End TTS System의 이점
<box>
1. 어려운 기능설계의 필요성이 적다.
2. 더 쉽게 풍부한 Speaker, Language, Sentiment등의 욥션을 달수 있는데, 이는 Input의 조정이 모델의 초반부에서 일어나기 때문이며 이로인해 새로운 데이터에 대한 적응이 더 쉽게 일어난다.
</box>

주의점
<box>
1. 고도로 압축된 소스가 오디오로 분해되는데 이로 인해 같은 문장에 대해서도 아른 발음, 억양등이 생길 수 있다. 이는 신호 레벨에서 큰 변화에 대처할 수 있는 유연성을 학습시킴으로서 해결 가능하다.
2. 결과물이 연속적이며, 결과물의 길이가 Input보다 훨씬 길기 때문에 Error가 발생할 수도 있다.
</box>

## 코드
---
