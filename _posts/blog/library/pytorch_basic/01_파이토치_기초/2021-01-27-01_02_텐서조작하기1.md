---
layout: post
bigtitle: PyTorch로 배우는 딥러닝/머신러닝 생기초
subtitle: '01.02.텐서 다루기1'
categories:
    - blog
    - library
tags: PyTorch
comments: true
related_posts:
  - category/_posts/blog/library/2021-01-27-01_01_파이토치_패키지의_기본구성.md
  - category/_posts/blog/library/2021-01-27-01_02_텐서조작하기2.md
published: true
---

# 01.파이토치 기초(PyTorch Basic)
## 02.텐서 다루기(Tensor Manipulation)1
---

개요
> 벡터, 행렬, 텐서의 개념에 대해서 이해하고, Numpy와 파이토치로 벡터, 행렬, 텐서를 다루는 방법에 대해서 이해


+ **벡터, 행렬 그리고 텐서(Vector, Matrix and Tensor)**  

* **넘파이 훑어보기(Numpy Review)**

+ **파이토치 텐서 선언하기(PyTorch Tensor Allocation)**

+ **행렬 곱셈(Maxtrix Multiplication)**

+ **다른 오퍼레이션들(Other Basic Ops)**


## 02.1. 벡터, 행렬 그리고 텐서(Vector, Matrix and Tensor)
---
### 1) 벡터,행렬,텐서 그림으로 이해하기

![0102_01](/assets/img/Blog/library/pytorch_basic/0102_01.JPG)

딥러닝을 하게 되면 다루게 되는 가장 기본적인 단위: 벡터, 행렬, 텐서

스칼라 : 차원이 없는 값  
벡터(Vector) : 1차원으로 구성된 값  
행렬(Matrix) : 2차원으로 구성된 값
텐서(Tensor) : 3차원으로 구성된 값  
4차원 : 텐서를 쌓아 올린거  
5차원 : 4차원을 옆으로 확장한거  
6차원 : 5차원을 뒤로 확장한 모습  

### 2) PyTorch Tensor Shape Convention
딥러닝을 할때 행렬,텐서의 크기를 고려하는 것은 항상 중요.  
행렬과 텐서의 크기를 표현할 표기

**\*2D Tensor(Typical Simple Setting)**

|t| = (batch_size,dim)

![0102_02](/assets/img/Blog/library/pytorch_basic/0102_02.JPG){: width="400" height=auto}

2차원 텐서의 크기 |t|를 (batch size x dimension)으로 표현  
행의 크기 : batch_size  
열의 크기 : dim


**\*3D Tensor(Typical Computer Vision) - 비전 분야에서의 3차원 텐서**

|t| = (batch_size,width,height)

![0102_03](/assets/img/Blog/library/pytorch_basic/0102_03.JPG){: width="400" height=auto}

일반적으로 비전 분야(이미지, 영상 처리)  

세로 : batch_size  
가로 : 너비(width)  
높이 : height

**\*3D Tensor(Typical Natural Language Processing) - NLP 분야에서의 3차원 텐서**

|t| = (batch_size,length,dim)

![0102_04](/assets/img/Blog/library/pytorch_basic/0102_04.JPG){: width="400" height=auto}

자연어 처리 3차원 텐서(batch_size, 문장 길이, 단어 벡터의 차원) 사4

세로 : batch_size  
가로 : 너비(width)  
높이 : height

## 02.2. 넘파이로 텐서 만들기(벡터와 행렬 만들기)
---
np.array(행렬)
~~~python
import numpy as np
~~~

### 1) 1D with Numpy
1차원 벡터
~~~python
t = np.array([0., 1., 2., 3., 4., 5., 6.])
print(t)
~~~

1차원 벡터의 차원과 크기 출력
~~~python
print('차원 : ', t.ndim)  # 차원 : 1
print('크기: ', t.shape)  # 크기 : (7,)
~~~
+ .ndim : 몇 차원인지를 출력  
+ shape는 크기를 출력

### 1-1) Numpy 기초 이해하기

~~~python
print('t[0] t[1] t[-1] = ', t[0], t[1], t[-1])

# t[0] t[1] t[-1] =  0.0 1.0 6.0
~~~
t[0] : index 0번째 원소
t[1] : index 1번째 원소
t[-1] : index -1번째 원소 --> 가장 뒤에(마지막) 원소


**범위 지정으로 원소를 불러오기 : 슬라이싱(Slicing)**

+ t[시작 번호 : 끝 번호] : 시작 번호부터 끝 번호 전 번호까지

~~~python
# t : [0. 1. 2. 3. 4. 5. 6.]
print('t[2:5] t[4:-1]  = ', t[2:5], t[4:-1])
# t[2:5] = [2. 3. 4.]
# t[4:-1] = [4. 5.]
~~~

+ 시작 번호 또는 끝 번호를 생략해서 슬라이싱  
[시작 번호:끝 번호]
**시작 번호**가 생략 되어있으면 : 처음부터 끝번호까지  
**끝 번호**가 생략 되어있으면 : 시작번호부터 끝까지  
~~~python
# t : [0. 1. 2. 3. 4. 5. 6.]
print('t[:2] t[3:] = ', t[:2], t[3:])
# t[:2] = [0. 1. 2.]
# t[3:] = [3. 4. 5. 6.]
~~~

### 2) 2D with Numpy
2차원 벡터

~~~python
t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])
print(t)
~~~
~~~
[[ 1.  2.  3.]
 [ 4.  5.  6.]
 [ 7.  8.  9.]
 [10. 11. 12.]]
~~~

2차원 벡터의 차원과 크기 출력
~~~python
print('차원 : ', t.ndim)  # 차원 : 2
print('크기: ', t.shape)  # 크기 : (4,3)
~~~


## 02.3. 파이토치 텐서 선언하기(PyTorch Tensor Allocation)
---
~~~python
import torch
~~~

### 1) 1D with PyTorch
파이토치로 1차원 행2
~~~python
t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])
print(t)
~~~

~~~python
print(t.dim())  # 차원 : 1
print(t.shape)  # 크기 : [7]
print(t.size()) # 크기 : [7]
~~~

~~~python
print(t[0], t[1], t[-1])  # 인덱스로 접근
print(t[2:5], t[4:-1])    # 슬라이싱
print(t[:2], t[3:])       # 슬라이싱
~~~

~~~
tensor(0.) tensor(1.) tensor(6.)
tensor([2., 3., 4.]) tensor([4., 5.])
tensor([0., 1.]) tensor([3., 4., 5., 6.])
~~~

### 2) 2D with PyTorch
파이토치로 2차원 행렬
~~~python
t = torch.FloatTensor([[1., 2., 3.],
                       [4., 5., 6.],
                       [7., 8., 9.],
                       [10., 11., 12.]
                      ])
print(t)
~~~

~~~python
print(t.dim())  # 차원 : 2
print(t.shape)  # 크기 : [4,3]
print(t.size()) # 크기 : [4,3]
~~~

슬라이싱
~~~python
print(t[:, 1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원의 첫번째 것만 가져온다.
print(t[:, 1].size()) # ↑ 위의 경우의 크기
~~~

~~~
tensor([ 2.,  5.,  8., 11.])
torch.Size([4])
~~~

슬라이싱 첫번째 차원은 전부, 두번째 차원은 처음부터 맨마지막 열 전열까지
~~~python
print(t[:, :-1])
~~~

~~~
tensor([[ 1.,  2.],
        [ 4.,  5.],
        [ 7.,  8.],
        [10., 11.]])
~~~


### 3) 브로드캐스팅(Broadcasting)

행렬의 덧셈/뺄셈 : 행렬의 크기가 같아야함  
행렬의 곱셈 : 앞 행렬의 마지막 차원 = 뒤 행렬의 마지막 차원 일치해야함

파이토치에서는 자동으로 행렬의 크기를 맞춰서 연산을 수행 : **브로드캐스팅**

~~~python
# Vector + scalar
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([3]) # [3] -> [3, 3]
print(m1 + m2)  # [1, 2] + [3, 3]
~~~

~~~
tensor([[4., 5.]])
~~~

~~~python
# 2 x 1 Vector + 1 x 2 Vector
m1 = torch.FloatTensor([[1, 2]])    # [[1, 2], [1, 2]]
m2 = torch.FloatTensor([[3], [4]])  # [[3, 3], [4, 4]]
print(m1 + m2)  # [[1+3, 2+3], [1+4, 2+4]]
~~~

~~~
tensor([4., 5.],
       [5., 6.]])
~~~

브로드캐스팅은 편리하지만, 자동으로 실행되는 기능이므로 주의가 필요 : 오류를 발생하지 않고 진행되기 때문에 실수를 넘겨버리고 결과가 바뀔수도 있음, 오류를 찾는것이 쉽지 않을 수 있음.


### 4) 자주 사용되는 기능들

#### 4-1) 행렬 곱셈과 원소별 곱셈의 차이(Matrix Multiplication Vs. Multiplication)
행렬 곱셈(.matmul)  
원소 별 곱셈(.mul)


**파이토치 텐서의 행렬 곱셈**

~~~python
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('m1 행렬의 크기: ', m1.shape) # 2 x 2
print('m2 행렬의 크기: ', m2.shape) # 2 x 1
print('m1과m2 곱', m1.matmul(m2)) # 2 x 1
~~~

$$ \begin{pmatrix} 1 & 2 \\ 3 & 4  \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} 5 \\ 11 \end{pmatrix}$$

~~~
Shape of Matrix 1:  torch.Size([2, 2])
Shape of Matrix 2:  torch.Size([2, 1])
tensor([[ 5.],
        [11.]])
~~~

**파이토치 텐서의 Element-wise 곱셈**
일한 크기의 행렬이 동일한 위치에 있는 원소끼리 곱  
서로 다른 크기의 행렬일 경우 브로드캐스팅이 된 후에 element-wise 곱셈이 수행

~~~python
m1 = torch.FloatTensor([[1, 2], [3, 4]])  
m2 = torch.FloatTensor([[1], [2]])  # [[1, 1], [2, 2]]
print('Shape of Matrix 1: ', m1.shape) # 2 x 2
print('Shape of Matrix 2: ', m2.shape) # 2 x 1
print(m1 * m2) # = print(m1.mul(m2))
~~~

$$ \begin{pmatrix} 1 & 2 \\ 3 & 4  \end{pmatrix} * \begin{pmatrix} 1 & 1\\ 2 & 2 \end{pmatrix} = \begin{pmatrix} 1 & 2\\ 6 & 8 \end{pmatrix}$$

~~~
Shape of Matrix 1:  torch.Size([2, 2])
Shape of Matrix 2:  torch.Size([2, 1])
tensor([[1., 2.],
        [6., 8.]])
tensor([[1., 2.],
        [6., 8.]])
~~~

#### 4-2) 덧셈(Sum)과 평균(Mean)
덧셈
dim=0 : 첫번째 차원(=행)을 의미
dim=1 : 두번째 차원(=열)을 의미
dim=-1 : 마지막 차원을 의미
~~~python
t = torch.FloatTensor([[1, 2], [3, 4]])

print(t.sum()) # 단순히 원소 전체의 덧셈을 수행
print(t.sum(dim=0)) # 행을 제거, 열을 기준으로 덧셈
print(t.sum(dim=1)) # 열을 제거, 행을 기준으로 덧셈
print(t.sum(dim=-1)) # 열을 제거(마지막 차원 제거)
~~~
~~~
tensor(10.)
tensor([4., 6.])
tensor([3., 7.])
tensor([3., 7.])
~~~

평균
~~~python
t = torch.FloatTensor([1, 2])
print(t.mean())
# 1.5000
~~~
~~~python
t = torch.FloatTensor([[1, 2], [3, 4]])

print(t.mean()) # 단순히 원소 전체의 평균 수행
print(t.mean(dim=0)) # 행을 제거, 열을 기준으로 평균
print(t.mean(dim=1)) # 열을 제거, 행을 기준으로 평균
print(t.mean(dim=-1)) # 열을 제거(마지막 차원 제거)
~~~
~~~
tensor(2.5000)  # 10/4
tensor([2., 3.])  # (1+3)/2, (2+4)/2
tensor([1.5000, 3.5000])  # (1+2)/2, (3+4)/2
tensor([1.5000, 3.5000])
~~~


#### 4-3) Max와 ArgMax

최대(Max): 원소의 최대값을 리턴  
아그맥스(ArgMax): 최대값을 가진 인덱스를 리턴

~~~python
t = torch.FloatTensor([[1, 2], [3, 4]])
print(t)
print(t.max()) # 원소 중 최대값인 4를 리턴
~~~
~~~
tensor([[1., 2.],
        [3., 4.]])
tensor(4.)
~~~

dim=0 : 첫번째 차원을 제거
~~~python
print(t.max(dim=0))
~~~
~~~
(tensor([3., 4.]), tensor([1, 1]))
~~~
+ max에 dim 인자를 주면 argmax도 함께 리턴  
tensor([3., 4.])는 가장 큰값을 갖는 행렬을 리턴
tensor([1, 1])는 가장 큰값을 갖는 위치를 리턴 : 첫번째 열에서 3의 인덱스는 1, 두번째 열에서 4의 인덱스는 1 --> [1, 1]


~~~python
print('Max: ', t.max(dim=0)[0])
print('Argmax: ', t.max(dim=0)[1])
~~~
~~~
Max:  tensor([3., 4.])
Argmax:  tensor([1, 1])
~~~

~~~python
print(t.max(dim=1))
print(t.max(dim=-1))
~~~
~~~
(tensor([2., 4.]), tensor([1, 1]))
(tensor([2., 4.]), tensor([1, 1]))
~~~

이어서 텐서를 조작하는 방법을 알아본다, 텐서 차원 변경, 크기변경, View, Squeeze, Unsqueeze, Concatenate, Stacking, In-place
