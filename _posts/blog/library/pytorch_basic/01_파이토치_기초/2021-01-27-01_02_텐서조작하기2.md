---
layout: post
bigtitle: PyTorch로 배우는 딥러닝/머신러닝 생기초
subtitle: '01.02.텐서 다루기2'
categories:
    - blog
    - library
tags: PyTorch
comments: true
related_posts:
  - category/_posts/blog/library/2021-01-27-01_02_텐서조작하기2.md
  - category/_posts/blog/library/2021-01-27-01_03_파이썬 클래스.md
published: true
---

# 01.파이토치 기초(PyTorch Basic)
## 02.텐서 조작하기(Tensor Manipulation)2
---

개요
> 이어서 텐서를 조작하는 방법을 알아본다, 텐서 차원 변경, 크기변경, View, Squeeze, Unsqueeze, Concatenate, Stacking, In-place Operation

#### 4-4) 뷰(View) - 원소의 수를 유지하면서 텐서의 크기 변경

파이토치 텐서의 뷰(View) : Numpy의 reshape과 같은 역할  
텐서의 크기를 변경

**x.view([?,?]) --> x행렬을 ?x?크기의 행렬로 reshape, 원소의 개수는 유지

3차원 텐서
~~~python
t = np.array([[[0, 1, 2],
               [3, 4, 5]],
              [[6, 7, 8],
               [9, 10, 11]]])
ft = torch.FloatTensor(t)
print(ft.shape)
~~~
~~~
torch.Size([2, 2, 3])
~~~
![0102_05](/assets/img/Blog/library/pytorch_basic/0102_05.JPG)


#### 4-4-1) 3차원 텐서에서 2차원 텐서로 변경

~~~python
print(ft.view([-1, 3])) # ft라는 텐서를 (?, 3)의 크기로 변경
print(ft.view([-1, 3]).shape)
~~~
~~~
tensor([[ 0.,  1.,  2.],
        [ 3.,  4.,  5.],
        [ 6.,  7.,  8.],
        [ 9., 10., 11.]])
torch.Size([4, 3])
~~~

view([-1, 3]) :  
-1: 첫번째 차원은 사용자가 잘 모르겠으니 파이토치에 맡기겠다는 의미  
3 : 번째 차원의 길이는 3을 가지도록 하라는 의미  
==> 차원 텐서를 2차원 텐서로 변경하되 (?, 3)의 크기로 변경 :  (4, 3)의 크기

(2, 2, 3) -> (2 × 2, 3) -> (4, 3)

#### 4-4-2) 3차원 텐서의 크기 변경
3차원 텐서에서 3차원 텐서로 차원은 유지하되, 크기(shape)를 바꿈

view로 텐서의 크기를 변경하더라도 원소의 수는 유지

 (2 × 2 × 3) 텐서를 (? × 1 × 3) 텐서로 변경 --> ?는 4  
2 × 2 × 3) = (? × 1 × 3) = 12를 만족해야함

~~~python
print(ft.view([-1, 1, 3]))
print(ft.view([-1, 1, 3]).shape)
~~~
~~~
tensor([[[ 0.,  1.,  2.]],
        [[ 3.,  4.,  5.]],
        [[ 6.,  7.,  8.]],
        [[ 9., 10., 11.]]])
torch.Size([4, 1, 3])
~~~

#### 4-5) 스퀴즈(Squeeze) - 1인 차원을 제거
---
스퀴즈 : 차원이 1인 경우에는 해당 차원을 제거  


~~~python
ft = torch.FloatTensor([[0], [1], [2]])
print(ft)
print(ft.shape)
~~~
~~~
tensor([[0.],
        [1.],
        [2.]])
torch.Size([3, 1])
~~~

~~~python
print(ft.squeeze())
print(ft.squeeze().shape)
~~~
~~~
tensor([0., 1., 2.])
torch.Size([3])
~~~
1이었던 두번째 차원이 제거되면서  (3,)의 크기를 가지는 텐서로 변경 : 1차원 벡터
(3x1) --> (3,)
\[\[0.],  
[1.],  
[2.]] --> [3]  

#### 4-6) 언스퀴즈(Unsqueeze) - 특정 위치에 1인 차원을 추가
---
특정 위치에 1인 차원을 추가

~~~python
ft = torch.Tensor([0, 1, 2])
print(ft.shape)
~~~
~~~
torch.Size([3])
~~~
**현재는 차원이 1개인 1차원 벡터**  
첫번째 차원에 1인 차원을 추가

**.unsqueeze(0)** : 0은 index 0으로 첫번째 차원을 의미--> 첫번째 차원에 1인 차원이 추가
 (3,) --> 첫번째 차원에 1인 차원을 추가하면 (1, 3)의 크기
**.unsqueeze(1)** : 1은 index 1로 두번째 차원을 의미--> 두번째 차원에 1인 차원이 추가
 (3,) --> 두번째 차원에 1인 차원을 추가하면 (3, 1)의 크기

**.unsqueeze(-1)** : -1은 마지막 index의 차원을 의미--> 마지막 차원에 1인 차원이 추가


~~~python
print(ft.unsqueeze(0)) # 인덱스가 0부터 시작하므로 0은 첫번째 차원을 의미한다.
print(ft.unsqueeze(0).shape)
~~~
~~~
tensor([[0., 1., 2.]])
torch.Size([1, 3])
~~~

(3,)의 크기를 가졌던 1차원 벡터 -> (1, 3)의 2차원 텐서로 변경

~~~python
print(ft.unsqueeze(1)) # 1은 두번째 차원을 의미한다.
print(ft.unsqueeze(1).shape)
~~~
~~~
tensor([[0.],
        [1.],
        [2.]])
torch.Size([3, 1])
~~~

(3,)의 크기를 가졌던 1차원 벡터 -> (3,1)의 2차원 텐서로 변경

~~~python
print(ft.unsqueeze(-1)) # -1은 마지막 차원을 의미 여기서는 두번째 차원
print(ft.unsqueeze(-1).shape)
~~~
~~~
tensor([[0., 1., 2.]])
torch.Size([1, 3])
~~~

(3,)의 크기를 가졌던 1차원 벡터 -> (3,1)의 2차원 텐서로 변경


**.view()** 로 1차원 벡터를 2차원 텐서로 만들기

~~~python
print(ft.view(1, -1))
print(ft.view(1, -1).shape)
~~~
~~~
tensor([[0., 1., 2.]])
torch.Size([1, 3])
~~~
unsqueeze랑 view랑 비슷~

#### 4-7) 타입 캐스팅(Type Casting)
---
![0102_06](/assets/img/Blog/library/pytorch_basic/0102_06.JPG)

자료형을 변환하는 것 : 타입 캐스팅

~~~python
lt = torch.LongTensor([1, 2, 3, 4])
print(lt)
~~~

~~~python
print(lt.float()) # long -> float
~~~

~~~python
bt = torch.ByteTensor([True, False, False, True])
print(bt)
~~~

~~~python
print(bt.long())  # long 타입의 텐서로 변경
print(bt.float()) # float 타입의 텐서로 변경
~~~

#### 4-8) 연결하기(Concatenate)
---
**torch.cat[x,y]** : x 텐서와 y 텐서를 연결,  
dim : 어느 차원을 늘릴 것인지 인자로 줄 수 있음, dim=0 : 첫번째 차원을 늘림

~~~python
x = torch.FloatTensor([[1, 2], [3, 4]])
y = torch.FloatTensor([[5, 6], [7, 8]])

print(torch.cat([x, y], dim=0))
~~~

~~~
tensor([[1., 2.],
        [3., 4.],
        [5., 6.],
        [7., 8.]])
~~~
dim=0 이면 두 개의 (2 × 2) 텐서 --> (4 x 2)


dim=1 이면
~~~python
print(torch.cat([x, y], dim=1))
~~~

~~~
tensor([[1., 2., 5., 6.],
        [3., 4., 7., 8.]])
~~~
dim=1 이면 두 개의 (2 × 2) 텐서 --> (2 × 4)

+ 딥 러닝에서는 주로 모델의 입력 또는 중간 연산에서 두 개의 텐서를 연결하는 경우가 많습니다. 두 텐서를 연결해서 입력으로 사용하는 것은 두 가지의 정보를 모두 사용한다는 의미를 가지고 있습니다.


#### 4-9) 스택킹(Stacking)
---
연결(concatenate)을 하는 또 다른 방법으로 스택킹(Stacking)

위에서 아래로 쌓기 : **torch.stack([x,y,z])**  
왼쪽에서 오른쪽으로 쌓기 : **torch.stack([x,y,z], dim=1)**

크기가 (2,)로 모두 동일한 3개의 벡터
~~~python
x = torch.FloatTensor([1, 4])
y = torch.FloatTensor([2, 5])
z = torch.FloatTensor([3, 6])
~~~
**torch.stack을 통해서 3개의 벡터를 모두 스택킹**

+ **torch.stack([x,y,z])**  
~~~python
print(torch.stack([x, y, z]))
~~~
~~~
tensor([[1., 4.],
        [2., 5.],
        [3., 6.]])
~~~

![0102_07](/assets/img/Blog/library/pytorch_basic/0102_07.JPG)

세 개의 (1 × 2) 텐서 아래로 stacking --> (3 × 2)

+ **torch.stack([x,y,z], dim=1)**
~~~python
print(torch.stack([x, y, z], dim=1))
~~~

~~~
tensor([[1., 2., 3.],
        [4., 5., 6.]])
~~~

![0102_08](/assets/img/Blog/library/pytorch_basic/0102_08.JPG)

세 개의 (1 × 2) 텐서 옆으로 stacking --> (2 × 3)

**torch.stack([x, y, z])**  
= **torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0)**


~~~python
print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0))
~~~

~~~
tensor([[1., 4.],
        [2., 5.],
        [3., 6.]])
~~~

#### 4-10) ones_like와 zeros_like - 0으로 채워진 텐서와 1로 채워진 텐서
---

(2 x 3) 텐서

~~~python
x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])
print(x)
~~~
~~~
tensor([[0., 1., 2.],
        [2., 1., 0.]])
~~~

 ones_like를 하면 동일한 크기(shape)의 1으로만 값이 채워진 텐서를 생성
~~~python
print(torch.ones_like(x))
~~~
~~~
tensor([[1., 1., 1.],
        [1., 1., 1.]])
~~~

zeros_like를 하면 동일한 크기(shape)의 0으로만 값이 채워진 텐서를 생성

~~~python
print(torch.zeros_like(x))
~~~
~~~
tensor([[0., 0., 0.],
        [0., 0., 0.]])

~~~

#### 4-11) In-place Operation (덮어쓰기 연산)

**연산 뒤에 _를 붙이면 기존의 값에 덮어쓰기**

(2 × 2) 텐서를 만들고 x에 저장


~~~python
x = torch.FloatTensor([[1, 2], [3, 4]])
~~~

곱하기 연산을 한 값과 기존의 값을 출력
~~~python
print(x.mul(2.)) # 곱하기 2를 수행한 결과를 출력
print(x) # 기존의 값 출력
~~~
~~~
tensor([[2., 4.],
        [6., 8.]])
tensor([[1., 2.],
        [3., 4.]])
~~~

첫번째 출력은 곱하기 2가 수행된 결과  
두번째 출력은 기존 값이 그대로 출력  

곱하기 2를 수행했지만 x에 다시 저장하지 않았으므로  
기존 값 x는 변하지 않음

연산 뒤에 _를 붙이면 기존의 값을 덮어쓰기를 함

~~~python
print(x.mul_(2.))  
print(x)
~~~
~~~
tensor([[2., 4.],
        [6., 8.]])
tensor([[2., 4.],
        [6., 8.]])
~~~
